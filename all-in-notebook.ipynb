{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IZXTX1HXuOdJ",
    "outputId": "3c91edda-7e41-4f34-9046-bfc73994445e",
    "tags": [
     "command"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'bachlor_google'...\n",
      "remote: Enumerating objects: 1133, done.\u001b[K\n",
      "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
      "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
      "remote: Total 1133 (delta 8), reused 12 (delta 3), pack-reused 1106 (from 1)\u001b[K\n",
      "Receiving objects: 100% (1133/1133), 66.92 MiB | 19.62 MiB/s, done.\n",
      "Resolving deltas: 100% (245/245), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mszczesniak02/bachlor_google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pcFdDSJruOdK",
    "tags": [
     "command"
    ]
   },
   "outputs": [],
   "source": [
    "!cp -r /content/bachlor_google/DeepCrack/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "3abHpmtZuOdL",
    "outputId": "0075df41-090b-4641-e244-cee9027cb909",
    "tags": [
     "command"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting segmentation-models-pytorch\n",
      "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.6.2)\n",
      "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.20)\n",
      "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.8.0+cu126)\n",
      "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.23.0+cu126)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.10)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.5)\n",
      "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
      "Successfully installed segmentation-models-pytorch-0.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install segmentation-models-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--P1NscFuOdM"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krzeslaav/bachlor/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-10-26 21:49:21.190863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A                              # for augmentation transform\n",
    "\n",
    "import numpy as np                                      # sci kit specials ;D\n",
    "import matplotlib.pyplot as plt                         # plots\n",
    "from PIL import Image                                   # for opening images as numpy arrays or torch tensors\n",
    "\n",
    "from datetime import datetime                           # for model timestamp\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import Dataset                    # preset class for creating a dataset\n",
    "from torch.utils.data import random_split               # for splitting datasets into training, test, validation\n",
    "from torch.utils.data import DataLoader                 # self-explanitory\n",
    "import segmentation_models_pytorch as smp               # preset model\n",
    "\n",
    "from tqdm import tqdm                                   # for the progress bar\n",
    "import os                                               # for accessing files and setting proper paths to   them\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter       # tensorboard srv\n",
    "from torch.nn.functional import binary_cross_entropy\n",
    "from torchvision.ops import sigmoid_focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvxDfZEXuOdO"
   },
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "if DEBUG==True:\n",
    "\n",
    "  MASK_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_lab\"\n",
    "  IMG_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_img\"\n",
    "  MASK_TEST_PATH = \"../assets/datasets/DeepCrack/test_lab\"\n",
    "  IMG_TEST_PATH = \"../assets/datasets/DeepCrack/test_img\"\n",
    "  DEVICE = \"cuda\"\n",
    "  BATCH_SIZE = 2\n",
    "  WORKERS = 2\n",
    "\n",
    "else:\n",
    "  MASK_TRAIN_PATH = \"/content/DeepCrack/train_lab\"\n",
    "  IMG_TRAIN_PATH = \"/content/DeepCrack/train_img\"\n",
    "  MASK_TEST_PATH = \"/content/DeepCrack/test_lab\"\n",
    "  IMG_TEST_PATH = \"/content/DeepCrack/test_img\"\n",
    "  BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "  DEVICE=\"cuda\"\n",
    "  WORKERS = 2\n",
    "\n",
    "PIN_MEMORY = True\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 40\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 15\n",
    "\n",
    "SCHEDULER_PATIENCE = 5\n",
    "SCHEDULER_FACTOR = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yshAUFtwuOdO"
   },
   "outputs": [],
   "source": [
    "class DeepCrackDataset(Dataset):\n",
    "  def __init__(self, img_dir, mask_dir, transform=None):\n",
    "\n",
    "    self.img_dir = img_dir\n",
    "    self.mask_dir = mask_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    # sort values so the file names corespoding to each other are loaded in order\n",
    "    self.images = sorted([os.path.join(img_dir, file) for file in os.listdir(img_dir)] )\n",
    "    self.masks = sorted([os.path.join(mask_dir, file) for file in os.listdir(mask_dir)])\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.images)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    np_image = np.array(Image.open(self.images[index]))\n",
    "    np_mask = np.array(Image.open(self.masks[index]))\n",
    "\n",
    "\n",
    "    if len(np_mask.shape) == 3:\n",
    "      np_mask = np_mask[:,:,0]\n",
    "\n",
    "    np_mask = (np_mask > 127).astype(np.uint8)\n",
    "\n",
    "    if self.transform: # if using transforms\n",
    "      t = self.transform(image=np_image, mask=np_mask)\n",
    "      np_image = t[\"image\"]\n",
    "      np_mask = t[\"mask\"]\n",
    "\n",
    "    # conversion from numpy array convention to tensor via permute,\n",
    "    #     then normalizing to [0,1] range, same for mask, only using binary data\n",
    "    tensor_image = torch.from_numpy(np_image).permute(2, 0, 1).float() / 255.0\n",
    "    tensor_mask = torch.from_numpy(np_mask).unsqueeze(0).float()\n",
    "\n",
    "    return tensor_image,tensor_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W7UY0hh3uOdP"
   },
   "outputs": [],
   "source": [
    "transform_train = A.Compose([\n",
    "    A.Resize(512, 512),  \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.3),\n",
    "    A.Rotate(limit=10, p=0.3),  \n",
    "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3), \n",
    "])\n",
    "\n",
    "transform_val = A.Compose([A.Resize(512,512)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrXVv0mLuOdQ"
   },
   "outputs": [],
   "source": [
    "class BCrossEntropyLoss(torch.nn.Module):\n",
    "  def __init__(self, smooth=1e-16): # zmiana\n",
    "    super(BCrossEntropyLoss,self).__init__()\n",
    "    self.smooth = smooth\n",
    "  def forward(self, predictions, targets):\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    loss = binary_cross_entropy(predictions, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(torch.nn.Module):\n",
    "  def __init__(self, smooth=1e-16): # zmiana\n",
    "    super(BCrossEntropyLoss,self).__init__()\n",
    "    self.smooth = smooth\n",
    "  def forward(self, predictions, targets):\n",
    "    loss =sigmoid_focal_loss(predictions,targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCE_with_FocalLoss(torch.nn.Module):\n",
    "  def __init__(self, smooth=1e-16): # zmiana\n",
    "    super(BCE_with_FocalLoss,self).__init__()\n",
    "    self.smooth = smooth\n",
    "  def forward(self, predictions, targets):\n",
    "    # loss_focal = sigmoid_focal_loss(predictions,targets)\n",
    "    # loss_bce = binary_cross_entropy(predictions, targets)\n",
    "    loss = 0.5 * sigmoid_focal_loss(predictions,targets)+ 0.5 * binary_cross_entropy(predictions, targets)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceLoss(torch.nn.Module):\n",
    "  def __init__(self, smooth=1e-6):\n",
    "    super(DiceLoss,self).__init__()\n",
    "    self.smooth = smooth\n",
    "  def forward(self, predictions, targets):\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "\n",
    "    predictions = predictions.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    intersection = (predictions * targets).sum()\n",
    "    dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
    "\n",
    "    return 1-dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceFocalLoss(torch.nn.Module):\n",
    "  def __init__(self, smooth=1e-6):\n",
    "    super(DiceLoss,self).__init__()\n",
    "    self.smooth = smooth\n",
    "  def forward(self, predictions, targets):\n",
    "    \n",
    "    focal_loss = sigmoid_focal_loss(predictions,targets)\n",
    "\n",
    "    predictions = torch.sigmoid(predictions)\n",
    "    predictions = predictions.view(-1)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    intersection = (predictions * targets).sum()\n",
    "    dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
    "    \n",
    "    dice_loss = 1-dice\n",
    "\n",
    "    the_loss = dice_loss * 0.5 + focal_loss * 0.5\n",
    "\n",
    "    return the_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xOyfdE20uOdS"
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions, targets, threshold=0.5):\n",
    "\n",
    "    # Binaryzacja\n",
    "    preds = (predictions > threshold).float()\n",
    "    targets = targets.float()\n",
    "\n",
    "    # Flatten\n",
    "    preds_flat = preds.view(-1)\n",
    "    targets_flat = targets.view(-1)\n",
    "\n",
    "    # True/False Positives/Negatives\n",
    "    TP = ((preds_flat == 1) & (targets_flat == 1)).sum().float()\n",
    "    TN = ((preds_flat == 0) & (targets_flat == 0)).sum().float()\n",
    "    FP = ((preds_flat == 1) & (targets_flat == 0)).sum().float()\n",
    "    FN = ((preds_flat == 0) & (targets_flat == 1)).sum().float()\n",
    "\n",
    "    conf_table = [[TP,FP],[FN, TN]]\n",
    "\n",
    "    # Metryki\n",
    "    epsilon = 1e-7  # Unikaj dzielenia przez zero\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
    "    precision = TP / (TP + FP + epsilon)\n",
    "    recall = TP / (TP + FN + epsilon)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
    "    specificity = TN / (TN + FP + epsilon)\n",
    "\n",
    "    # IoU (Intersection over Union) - NAJWAŻNIEJSZA dla segmentacji!\n",
    "    intersection = (preds * targets).sum()\n",
    "    union = preds.sum() + targets.sum() - intersection\n",
    "    iou = intersection / (union + epsilon)\n",
    "\n",
    "    # Dice Coefficient\n",
    "    dice = (2 * intersection) / (preds.sum() + targets.sum() + epsilon)\n",
    "\n",
    "    return {\n",
    "        'accuracy': accuracy.item(),\n",
    "        'precision': precision.item(),\n",
    "        'recall': recall.item(),\n",
    "        'f1_score': f1_score.item(),\n",
    "        'specificity': specificity.item(),\n",
    "        'iou': iou.item(),\n",
    "        'dice': dice.item(),\n",
    "        'confusion_table': conf_table\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N3IY_qYBuOdT"
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = .0\n",
    "    metrics = {\n",
    "        'iou': [], 'dice': [], 'recall': [],\n",
    "        'precision': [], 'f1_score': []\n",
    "    }\n",
    "\n",
    "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for batch_idx, (images, masks) in enumerate(loop):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        # Forward pass\n",
    "        predictions = model(images)\n",
    "        loss = criterion(predictions, masks)\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predictions_sigmoid = torch.sigmoid(predictions)\n",
    "            batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
    "\n",
    "            for key in metrics.keys():\n",
    "                value = batch_metrics[key]\n",
    "                if isinstance(value, torch.Tensor):\n",
    "                    value = value.cpu().item()\n",
    "                metrics[key].append(batch_metrics[key])\n",
    "\n",
    "        loop.set_postfix({'loss': loss.item()})\n",
    "\n",
    "    loop.close()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "    avg_metrics['loss'] = avg_loss\n",
    "\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "def validate(model, val_loader, criterion,device):\n",
    "  model.eval()\n",
    "  running_loss = 0.0\n",
    "  metrics = {\n",
    "        'iou': [], 'dice': [], 'recall': [],\n",
    "        'precision': [], 'f1_score': []\n",
    "    }\n",
    "  with torch.no_grad():\n",
    "    for images,masks in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
    "      \n",
    "      images = images.to(device)\n",
    "      masks = masks.to(device)\n",
    "\n",
    "      predictions = model(images)\n",
    "      loss = criterion(predictions, masks)\n",
    "\n",
    "      running_loss += loss.item()\n",
    "\n",
    "      predictions_sigmoid = torch.sigmoid(predictions)\n",
    "      batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
    "\n",
    "      for key in metrics.keys():\n",
    "          value = batch_metrics[key]\n",
    "\n",
    "          if isinstance(value, torch.Tensor):\n",
    "              value = value.cpu().item()\n",
    "\n",
    "          metrics[key].append(batch_metrics[key])\n",
    "\n",
    "    avg_loss = running_loss / len(val_loader)\n",
    "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
    "    avg_metrics['loss'] = avg_loss\n",
    "\n",
    "    return avg_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MajrjmMsuOdU"
   },
   "outputs": [],
   "source": [
    "def main()-> int:\n",
    "\n",
    "    its_training_time = datetime.now().strftime('H%M') # Jared Leto likes it\n",
    "    writer = SummaryWriter(f\"runs/model_{its_training_time}\")\n",
    "\n",
    "    train_set = DeepCrackDataset(IMG_TRAIN_PATH, MASK_TRAIN_PATH, transform_train=transform_train)\n",
    "    val_dataset = DeepCrackDataset(IMG_TEST_PATH, MASK_TEST_PATH, transform_train=transform_val)\n",
    "\n",
    "    test_set, val_set =  random_split(val_dataset, [.2, .8])\n",
    "\n",
    "    train_loader    = DataLoader( train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
    "    test_loader     = DataLoader( test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
    "    val_loader      = DataLoader( val_set, batch_size=BATCH_SIZE , shuffle=False, num_workers=WORKERS    , pin_memory=PIN_MEMORY)\n",
    "\n",
    "    print(\"Datasets sizes: \")\n",
    "    print(f\"\\t  Train: {len(train_set)}\")\n",
    "    print(f\"\\t    Val: {len(val_set)}\")\n",
    "    print(f\"\\t   Test: {len(test_set)}\")\n",
    "\n",
    "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet34\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = DiceFocalLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='max',  # Maksymalizuj IoU\n",
    "        factor=SCHEDULER_FACTOR,\n",
    "        patience=SCHEDULER_PATIENCE,\n",
    "        min_lr=1e-7\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTraining configuration:\")\n",
    "    print(f\"\\t       Optimizer: Adam\")\n",
    "    print(f\"\\t   Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"\\t    Weight decay: {WEIGHT_DECAY}\")\n",
    "    print(f\"\\t       Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE})\")\n",
    "    print(f\"\\t  Early stopping: patience={EARLY_STOPPING_PATIENCE}\")\n",
    "    print(f\"\\t          Epochs: {EPOCHS}\")\n",
    "\n",
    "    epochs = EPOCHS\n",
    "    best_val_iou = 0.0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "\n",
    "    writer.add_text(\"Hparams\",f\"\"\"\n",
    "    -           Learning Rate: {LEARNING_RATE}\n",
    "    -              Batch Size: {BATCH_SIZE}\n",
    "    -            Weight Decay: {WEIGHT_DECAY}\n",
    "    -                  Epochs: {EPOCHS}\n",
    "    -      Scheduler Patience: {SCHEDULER_PATIENCE}\n",
    "    - Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\n",
    "    -                  Device: {DEVICE}\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Epochs:{epochs}, current:\",end='')\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"{epoch},\")\n",
    "\n",
    "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_metrics = validate(model, val_loader, criterion, device)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        scheduler.step(val_metrics['iou'])\n",
    "\n",
    "        # Loss\n",
    "        writer.add_scalars('Loss', {\n",
    "            'train': train_metrics['loss'],\n",
    "            'val': val_metrics['loss']\n",
    "        }, epoch)\n",
    "\n",
    "        # IoU\n",
    "        writer.add_scalars('IoU', {\n",
    "            'train': train_metrics['iou'],\n",
    "            'val': val_metrics['iou']\n",
    "        }, epoch)\n",
    "\n",
    "        # Dice\n",
    "        writer.add_scalars('Dice', {\n",
    "            'train': train_metrics['dice'],\n",
    "            'val': val_metrics['dice']\n",
    "        }, epoch)\n",
    "\n",
    "        # Precision\n",
    "        writer.add_scalars('Precision', {\n",
    "            'train': train_metrics['precision'],\n",
    "            'val': val_metrics['precision']\n",
    "        }, epoch)\n",
    "\n",
    "        # Recall\n",
    "        writer.add_scalars('Recall', {\n",
    "            'train': train_metrics['recall'],\n",
    "            'val': val_metrics['recall']\n",
    "        }, epoch)\n",
    "\n",
    "        # F1-Score\n",
    "        writer.add_scalars('F1-Score', {\n",
    "            'train': train_metrics['f1_score'],\n",
    "            'val': val_metrics['f1_score']\n",
    "        }, epoch)\n",
    "\n",
    "        # Learning Rate\n",
    "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
    "\n",
    "        # # Accuracy (tylko validation)\n",
    "        # writer.add_scalar('Validation/Accuracy', val_metrics['accuracy'], epoch)\n",
    "\n",
    "        # SAVE BEST MODEL\n",
    "        if val_metrics['iou'] > best_val_iou:\n",
    "            best_val_iou = val_metrics['iou']\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_iou': best_val_iou,\n",
    "                'val_metrics': val_metrics,\n",
    "                'train_metrics': train_metrics,\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint, f'best_model_iou_{best_val_iou:.4f}.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # ========================================\n",
    "        # EARLY STOPPING\n",
    "        # ========================================\n",
    "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
    "            print(f\"\\n  Early stopping triggered!\")\n",
    "            print(f\"   No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
    "            print(f\"   Best IoU: {best_val_iou:.4f} at epoch {best_epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    \n",
    "        val_images, val_masks = next(iter(val_loader))\n",
    "        val_images = val_images.to(device)\n",
    "        val_masks = val_masks.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_outputs = model(val_images)\n",
    "            val_preds = torch.sigmoid(val_outputs)\n",
    "\n",
    "        # Weź pierwsze 4 obrazy\n",
    "        writer.add_images('Images/Input', val_images[:4], epoch)\n",
    "        writer.add_images('Images/Ground_Truth', val_masks[:4], epoch)\n",
    "        writer.add_images('Images/Prediction', val_preds[:4], epoch)\n",
    "\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Training done.\")\n",
    "\n",
    "\n",
    "#     # Load best model\n",
    "    checkpoint = torch.load(f'best_model_iou_{best_val_iou:.4f}.pth', weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    test_metrics = validate(model, test_loader, criterion, device)\n",
    "#      # Zapisz hyperparametry vs metryki (do porównania w TensorBoard)\n",
    "#     writer.add_hparams(\n",
    "#         {\n",
    "#             'lr': LEARNING_RATE,\n",
    "#             'batch_size': BATCH_SIZE,\n",
    "#             'weight_decay': WEIGHT_DECAY,\n",
    "#             'scheduler_patience': SCHEDULER_PATIENCE,\n",
    "#         },\n",
    "#         {\n",
    "#             'hparam/test_iou': test_metrics['iou'],\n",
    "#             'hparam/test_dice': test_metrics['dice'],\n",
    "#             'hparam/test_f1': test_metrics['f1_score'],\n",
    "#         }\n",
    "#     )\n",
    "\n",
    "#     writer.add_text('Final_Test_Metrics', f\"\"\"\n",
    "    # Test Set Results (Best Model from Epoch {best_epoch + 1})\n",
    "\n",
    "# -       IoU: {test_metrics['iou']:.4f}\n",
    "# -      Dice: {test_metrics['dice']:.4f}\n",
    "# -    Recall: {test_metrics['recall']:.4f}\n",
    "# - Precision: {test_metrics['precision']:.4f}\n",
    "# -  F1-Score: {test_metrics['f1_score']:.4f}\n",
    "# Best Validation IoU: {best_val_iou:.4f}\n",
    "#     \"\"\")\n",
    "\n",
    "#     # Dodaj finalne metryki jako skalary\n",
    "#     writer.add_scalar('Final/Test_IoU', test_metrics['iou'], 0)\n",
    "#     writer.add_scalar('Final/Test_Dice', test_metrics['dice'], 0)\n",
    "#     writer.add_scalar('Final/Test_Recall', test_metrics['recall'], 0)\n",
    "#     writer.add_scalar('Final/Test_Precision', test_metrics['precision'], 0)\n",
    "#     writer.add_scalar('Final/Test_F1', test_metrics['f1_score'], 0)\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"\\nTest Results:\")\n",
    "    print(f\"   IoU:       {test_metrics['iou']:.4f}\")\n",
    "    print(f\"   Dice:      {test_metrics['dice']:.4f}\")\n",
    "    print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
    "    print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
    "\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKAh-oh7vQmJ",
    "outputId": "35b50ebb-7a80-4ac5-b9d4-89ebf58c673f"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmain\u001b[39m()-> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     its_training_time = \u001b[43mdatetime\u001b[49m.now().strftime(\u001b[33m'\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;66;03m# Jared Leto likes it\u001b[39;00m\n\u001b[32m      4\u001b[39m     writer = SummaryWriter(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mruns/model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mits_training_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     dataset = get_dataset(IMG_TRAIN_PATH, MASK_TRAIN_PATH, transform_train=\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "ARRwxkmA0i52"
   },
   "outputs": [],
   "source": [
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRrCX8OHk8-k",
    "outputId": "1c7563f6-5e07-4f6c-d81f-4ec03f4c4e1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated before clearing cache: 417080320 bytes\n",
      "Memory allocated after clearing cache: 417080320 bytes\n"
     ]
    }
   ],
   "source": [
    "print(f\"Memory allocated before clearing cache: {torch.cuda.memory_allocated()} bytes\")\n",
    "\n",
    "# Manually invoke garbage collection\n",
    "\n",
    "# Clear GPU cache\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(f\"Memory allocated after clearing cache: {torch.cuda.memory_allocated()} bytes\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "bachlor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
