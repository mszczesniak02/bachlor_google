{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "command"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZXTX1HXuOdJ",
        "outputId": "4c0b74de-819d-4547-d5e0-4f5e1569817a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bachlor_google'...\n",
            "remote: Enumerating objects: 1112, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 1112 (delta 1), reused 5 (delta 1), pack-reused 1106 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1112/1112), 66.90 MiB | 14.71 MiB/s, done.\n",
            "Resolving deltas: 100% (238/238), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mszczesniak02/bachlor_google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "command"
        ],
        "id": "pcFdDSJruOdK"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/bachlor_google/DeepCrack/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "tags": [
          "command"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3abHpmtZuOdL",
        "outputId": "975c03f2-1de2-4b62-dc17-87eebc0d60a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.20)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.5)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "--P1NscFuOdM"
      },
      "outputs": [],
      "source": [
        "import albumentations as A                              # for augmentation transform\n",
        "\n",
        "import numpy as np                                      # sci kit specials ;D\n",
        "import matplotlib.pyplot as plt                         # plots\n",
        "from PIL import Image                                   # for opening images as numpy arrays or torch tensors\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset                    # preset class for creating a dataset\n",
        "from torch.utils.data import random_split               # for splitting datasets into training, test, validation\n",
        "from torch.utils.data import DataLoader                 # self-explanitory\n",
        "import segmentation_models_pytorch as smp               # preset model\n",
        "\n",
        "from tqdm import tqdm                                   # for the progress bar\n",
        "import os                                               # for accessing files and setting proper paths to   them\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter       # tensorboard srv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XvxDfZEXuOdO"
      },
      "outputs": [],
      "source": [
        "DEBUG = False\n",
        "\n",
        "if DEBUG==True:\n",
        "\n",
        "  MASK_PATH = \"../assets/datasets/DeepCrack/train_lab\"\n",
        "  IMAGE_PATH = \"../assets/datasets/DeepCrack/train_img\"\n",
        "  DEVICE = \"cpu\"\n",
        "  WORKERS = 4\n",
        "\n",
        "else:\n",
        "  MASK_PATH = \"/content/DeepCrack/train_lab\"\n",
        "  IMAGE_PATH = \"/content/DeepCrack/train_img\"\n",
        "  DEVICE=\"cuda\"\n",
        "  WORKERS = 4\n",
        "\n",
        "BATCH_SIZE = 8\n",
        "PIN_MEMORY = True\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 10\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "\n",
        "SCHEDULER_PATIENCE = 5\n",
        "SCHEDULER_FACTOR = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yshAUFtwuOdO"
      },
      "outputs": [],
      "source": [
        "def fetch_data(path) -> list:\n",
        "  \"\"\"Return files as their paths+filename in an array\"\"\"\n",
        "\n",
        "  assert (os.path.exists(path) == True),  \"Failure during data fetching\"\n",
        "\n",
        "  result = []\n",
        "  for file in tqdm(os.listdir(path), desc=f\"Loading files from {path} \",unit=\"File\", leave=True):\n",
        "    fpath = os.path.join(path,file)\n",
        "    result.append(fpath)\n",
        "\n",
        "  return result\n",
        "\n",
        "\n",
        "class DeepCrackDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, transform=None):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    # sort values so the file names corespoding to each other are loaded in order\n",
        "    self.images = sorted([os.path.join(img_dir, file) for file in os.listdir(img_dir)] )\n",
        "    self.masks = sorted([os.path.join(mask_dir, file) for file in os.listdir(mask_dir)])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    np_image = np.array(Image.open(self.images[index]))\n",
        "    np_mask = np.array(Image.open(self.masks[index]))\n",
        "\n",
        "\n",
        "    if len(np_mask.shape) == 3:\n",
        "      np_mask = np_mask[:,:,0]\n",
        "\n",
        "    np_mask = (np_mask > 127).astype(np.uint8)\n",
        "\n",
        "    if self.transform: # if using transforms\n",
        "      t = self.transform(image=np_image, mask=np_mask)\n",
        "      np_image = t[\"image\"]\n",
        "      np_mask = t[\"mask\"]\n",
        "\n",
        "    # conversion from numpy array convention to tensor via permute,\n",
        "    #     then normalizing to [0,1] range, same for mask, only using binary data\n",
        "    tensor_image = torch.from_numpy(np_image).permute(2, 0, 1).float() / 255.0\n",
        "    tensor_mask = torch.from_numpy(np_mask).unsqueeze(0).float()\n",
        "\n",
        "    return tensor_image,tensor_mask\n",
        "\n",
        "\n",
        "def get_dataset(img_path = IMAGE_PATH, mask_path = MASK_PATH ):\n",
        "\n",
        "  dataset = DeepCrackDataset(img_path, mask_path, transform=transofrm_train)\n",
        "  return dataset\n",
        "\n",
        "def split_dataset(dataset: DeepCrackDataset, train_factor, test_factor, val_factor )->list:\n",
        "  \"\"\"Split exising dataset given percentages as [0,1] floats, return list of  \"\"\"\n",
        "  train_set_len, test_set_len, val_set_len = int(dataset.__len__() * train_factor), int(dataset.__len__() * test_factor) , int(dataset.__len__() * val_factor)\n",
        "  train_set, test_set ,val_set = random_split(dataset, [train_set_len, test_set_len, val_set_len])\n",
        "\n",
        "  return [train_set, test_set, val_set]\n",
        "\n",
        "def show_dataset(data_loader, samples=4):\n",
        "    counter = 0\n",
        "    for images, masks in data_loader:\n",
        "        fig, axes = plt.subplots(samples, 2, figsize=(8, 12))\n",
        "        for i in range( samples ):\n",
        "\n",
        "            img = images[i].permute(1, 2, 0).numpy()\n",
        "            axes[i, 0].imshow(img)\n",
        "            axes[i, 0].set_title(f\"Image {i+1}\")\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            # # Maska\n",
        "            mask = masks[i, 0].numpy()\n",
        "            axes[i, 1].imshow(mask, cmap='gray')\n",
        "            axes[i, 1].set_title(f\"Mask {i+1}\")\n",
        "            axes[i, 1].axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        counter+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "W7UY0hh3uOdP"
      },
      "outputs": [],
      "source": [
        "transofrm_train = A.Compose([\n",
        "    A.Resize(512, 512),  # ← Stały rozmiar, bez crop\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.Rotate(limit=10, p=0.3),  # ← Mniejszy kąt\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),  # ← Mniej agresywne\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xrXVv0mLuOdQ"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-6):\n",
        "    super(DiceLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "\n",
        "    predictions = predictions.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (predictions * targets).sum()\n",
        "    dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "    return 1-dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xOyfdE20uOdS"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions, targets, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Oblicz metryki segmentacji dla pojedynczego batcha\n",
        "\n",
        "    Args:\n",
        "        predictions: tensor [B, 1, H, W] - output z modelu (po sigmoid)\n",
        "        targets: tensor [B, 1, H, W] - ground truth maski\n",
        "        threshold: próg binaryzacji (default 0.5)\n",
        "\n",
        "    Returns:\n",
        "        dict z metrykami\n",
        "    \"\"\"\n",
        "    # Binaryzacja\n",
        "    preds = (predictions > threshold).float()\n",
        "    targets = targets.float()\n",
        "\n",
        "    # Flatten\n",
        "    preds_flat = preds.view(-1)\n",
        "    targets_flat = targets.view(-1)\n",
        "\n",
        "    # True/False Positives/Negatives\n",
        "    TP = ((preds_flat == 1) & (targets_flat == 1)).sum().float()\n",
        "    TN = ((preds_flat == 0) & (targets_flat == 0)).sum().float()\n",
        "    FP = ((preds_flat == 1) & (targets_flat == 0)).sum().float()\n",
        "    FN = ((preds_flat == 0) & (targets_flat == 1)).sum().float()\n",
        "\n",
        "    # Metryki\n",
        "    epsilon = 1e-7  # Unikaj dzielenia przez zero\n",
        "\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "    specificity = TN / (TN + FP + epsilon)\n",
        "\n",
        "    # IoU (Intersection over Union) - NAJWAŻNIEJSZA dla segmentacji!\n",
        "    intersection = (preds * targets).sum()\n",
        "    union = preds.sum() + targets.sum() - intersection\n",
        "    iou = intersection / (union + epsilon)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * intersection) / (preds.sum() + targets.sum() + epsilon)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'f1_score': f1_score.item(),\n",
        "        'specificity': specificity.item(),\n",
        "        'iou': iou.item(),\n",
        "        'dice': dice.item(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N3IY_qYBuOdT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = .0\n",
        "    metrics = {\n",
        "        'iou': [], 'dice': [], 'recall': [],\n",
        "        'precision': [], 'f1_score': []\n",
        "    }\n",
        "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        # move to adequete memory\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions_sigmoid = torch.sigmoid(predictions)\n",
        "            batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "            for key in metrics.keys():\n",
        "                metrics[key].append(batch_metrics[key])\n",
        "\n",
        "        loop.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    loop.close()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "def validate(model, val_loader, criterion,device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  metrics = {\n",
        "      'iou': [], 'dice': [], 'recall': [],\n",
        "      'precision': [], 'f1_score': [], 'accuracy': []\n",
        "      }\n",
        "  with torch.no_grad():\n",
        "    for images,masks in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "      images = images.to(device)\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      predictions = model(images)\n",
        "      loss = criterion(predictions, masks)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      predictions_sigmoid = torch.sigmoid(predictions)\n",
        "      batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "      for key in metrics.keys():\n",
        "          metrics[key].append(batch_metrics[key])\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "\n",
        "\n",
        "def make_checkpoint(model, optimizer, best_val_loss, training_loss):\n",
        "   torch.save({\n",
        "      'epoch': EPOCHS,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'train_loss': training_loss,\n",
        "      'val_loss': best_val_loss,} , 'unet_MODEL_save.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Kompletna ewaluacja modelu\n",
        "\n",
        "    Returns:\n",
        "        metrics: dict z metrykami\n",
        "        predictions: array predykcji [N, H, W]\n",
        "        ground_truths: array prawdziwych masek [N, H, W]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "\n",
        "    print(\"Running evaluation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            predictions = torch.sigmoid(outputs)\n",
        "\n",
        "            # Do CPU\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_ground_truths.append(masks.cpu().numpy())\n",
        "\n",
        "    # Concatenate\n",
        "    predictions = np.concatenate(all_predictions, axis=0)[:, 0]  # [N, H, W]\n",
        "    ground_truths = np.concatenate(all_ground_truths, axis=0)[:, 0]  # [N, H, W]\n",
        "\n",
        "    # Binaryzacja\n",
        "    pred_binary = (predictions > threshold).astype(np.float32)\n",
        "    gt_binary = (ground_truths > 0.5).astype(np.float32)\n",
        "\n",
        "    # ========================================\n",
        "    # OBLICZ METRYKI\n",
        "    # ========================================\n",
        "    ious = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "\n",
        "    for pred, gt in zip(pred_binary, gt_binary):\n",
        "        # Confusion matrix\n",
        "        tp = (pred * gt).sum()\n",
        "        fp = (pred * (1 - gt)).sum()\n",
        "        fn = ((1 - pred) * gt).sum()\n",
        "        tn = ((1 - pred) * (1 - gt)).sum()\n",
        "\n",
        "        # IoU (Intersection over Union)\n",
        "        iou = tp / (tp + fp + fn + 1e-6)\n",
        "        ious.append(iou)\n",
        "\n",
        "        # Precision (jakość detekcji)\n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        precisions.append(precision)\n",
        "\n",
        "        # Recall (czułość)\n",
        "        recall = tp / (tp + fn + 1e-6)\n",
        "        recalls.append(recall)\n",
        "\n",
        "        # F1 Score (harmonic mean)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    metrics = {\n",
        "        'iou_mean': np.mean(ious),\n",
        "        'iou_std': np.std(ious),\n",
        "        'iou_median': np.median(ious),\n",
        "        'precision_mean': np.mean(precisions),\n",
        "        'precision_std': np.std(precisions),\n",
        "        'recall_mean': np.mean(recalls),\n",
        "        'recall_std': np.std(recalls),\n",
        "        'f1_mean': np.mean(f1_scores),\n",
        "        'f1_std': np.std(f1_scores),\n",
        "        'accuracy_mean': np.mean(accuracies),\n",
        "        'ious': ious,\n",
        "        'precisions': precisions,\n",
        "        'recalls': recalls,\n",
        "        'f1_scores': f1_scores,\n",
        "    }\n",
        "\n",
        "    return metrics, predictions, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MajrjmMsuOdU"
      },
      "outputs": [],
      "source": [
        "def main()-> int:\n",
        "\n",
        "    dataset = get_dataset(IMAGE_PATH, MASK_PATH)\n",
        "    [train_set, test_set, val_set] = split_dataset(dataset, .8, .1, .1)\n",
        "\n",
        "    train_loader = DataLoader( train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader( test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader = DataLoader( val_set, batch_size=BATCH_SIZE , shuffle=False, num_workers=WORKERS    , pin_memory=PIN_MEMORY)\n",
        "\n",
        "    print(\"Loading datasets...\")\n",
        "    print(f\"   Train: {len(train_set)} images\")\n",
        "    print(f\"   Val:   {len(val_set)} images\")\n",
        "    print(f\"   Test:  {len(test_set)} images\")\n",
        "\n",
        "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"   Total parameters: {total_params:,}\")\n",
        "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"   Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
        "\n",
        "\n",
        "\n",
        "    criterion = DiceLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',  # Maksymalizuj IoU\n",
        "        factor=SCHEDULER_FACTOR,\n",
        "        patience=SCHEDULER_PATIENCE,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining configuration:\")\n",
        "    print(f\"   Optimizer: Adam\")\n",
        "    print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "    print(f\"   Weight decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"   Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE})\")\n",
        "    print(f\"   Early stopping: patience={EARLY_STOPPING_PATIENCE}\")\n",
        "    print(f\"   Epochs: {EPOCHS}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    best_val_iou = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"\\n{'=' * 80}\")\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "        print(f\"{'=' * 80}\")\n",
        "\n",
        "        # TRAIN\n",
        "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # VALIDATE\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # SCHEDULER\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_metrics['iou'])\n",
        "\n",
        "\n",
        "\n",
        "       # ========================================\n",
        "        # PRINT METRICS\n",
        "        # ========================================\n",
        "        print(f\"\\nTraining:\")\n",
        "        print(f\"   Loss: {train_metrics['loss']:.4f}\")\n",
        "        print(f\"   IoU:  {train_metrics['iou']:.4f}\")\n",
        "        print(f\"   Dice: {train_metrics['dice']:.4f}\")\n",
        "\n",
        "        print(f\"\\nValidation:\")\n",
        "        print(f\"   Loss:      {val_metrics['loss']:.4f}\")\n",
        "        print(f\"   IoU:       {val_metrics['iou']:.4f} {'✅ NEW BEST!' if val_metrics['iou'] > best_val_iou else ''}\")\n",
        "        print(f\"   Dice:      {val_metrics['dice']:.4f}\")\n",
        "        print(f\"   Recall:    {val_metrics['recall']:.4f}\")\n",
        "        print(f\"   Precision: {val_metrics['precision']:.4f}\")\n",
        "        print(f\"   F1-Score:  {val_metrics['f1_score']:.4f}\")\n",
        "\n",
        "        print(f\"\\n LR: {current_lr:.6f}\")\n",
        "\n",
        "        # ========================================\n",
        "        # SAVE BEST MODEL\n",
        "        # ========================================\n",
        "        if val_metrics['iou'] > best_val_iou:\n",
        "            best_val_iou = val_metrics['iou']\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_iou': best_val_iou,\n",
        "                'val_metrics': val_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, f'best_model_iou_{best_val_iou:.4f}.pth')\n",
        "            print(f\"\\n Model saved: best_model_iou_{best_val_iou:.4f}.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"\\n No improvement for {patience_counter} epoch(s)\")\n",
        "\n",
        "        # ========================================\n",
        "        # EARLY STOPPING\n",
        "        # ========================================\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n  Early stopping triggered!\")\n",
        "            print(f\"   No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "            print(f\"   Best IoU: {best_val_iou:.4f} at epoch {best_epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    # ========================================\n",
        "    # FINAL EVALUATION ON TEST SET\n",
        "    # ========================================\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"Final evaluation on test set...\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f'best_model_iou_{best_val_iou:.4f}.pth', weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    test_metrics = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"   IoU:       {test_metrics['iou']:.4f}\")\n",
        "    print(f\"   Dice:      {test_metrics['dice']:.4f}\")\n",
        "    print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
        "    print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
        "    print(f\"   Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    # Save to TensorBoard\n",
        "    # writer.add_text('Final_Test_Metrics', str(test_metrics))\n",
        "\n",
        "    # writer.close()\n",
        "\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\" TRAINING COMPLETED!\")\n",
        "    print(f\"   Best Validation IoU: {best_val_iou:.4f} (epoch {best_epoch + 1})\")\n",
        "    print(f\"   Test IoU: {test_metrics['iou']:.4f}\")\n",
        "    print(f\"   Model saved as: best_model_iou_{best_val_iou:.4f}.pth\")\n",
        "    print(f\"{'=' * 80}\\n\")\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "xKAh-oh7vQmJ",
        "outputId": "843c5acd-1826-4e8b-8025-68073aa3d75b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "   Train: 240 images\n",
            "   Val:   30 images\n",
            "   Test:  30 images\n",
            "   Total parameters: 24,436,369\n",
            "   Trainable parameters: 24,436,369\n",
            "   Model size: ~97.7 MB\n",
            "\n",
            "Training configuration:\n",
            "   Optimizer: Adam\n",
            "   Learning rate: 0.0001\n",
            "   Weight decay: 1e-05\n",
            "   Scheduler: ReduceLROnPlateau (patience=5)\n",
            "   Early stopping: patience=15\n",
            "   Epochs: 10\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 128.12 MiB is free. Process 16743 has 14.61 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 67.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-451043146.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-948851342.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m# TRAIN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# VALIDATE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2033538355.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/base/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mdecoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/segmentation_models_pytorch/encoders/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depth\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 128.12 MiB is free. Process 16743 has 14.61 GiB memory in use. Of the allocated memory 14.43 GiB is allocated by PyTorch, and 67.44 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}