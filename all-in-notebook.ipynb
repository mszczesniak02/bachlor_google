{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZXTX1HXuOdJ",
        "outputId": "3c91edda-7e41-4f34-9046-bfc73994445e",
        "tags": [
          "command"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bachlor_google'...\n",
            "remote: Enumerating objects: 1133, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 1133 (delta 8), reused 12 (delta 3), pack-reused 1106 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1133/1133), 66.92 MiB | 19.62 MiB/s, done.\n",
            "Resolving deltas: 100% (245/245), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mszczesniak02/bachlor_google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcFdDSJruOdK",
        "tags": [
          "command"
        ]
      },
      "outputs": [],
      "source": [
        "!cp -r /content/bachlor_google/DeepCrack/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3abHpmtZuOdL",
        "outputId": "0075df41-090b-4641-e244-cee9027cb909",
        "tags": [
          "command"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.20)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.5)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--P1NscFuOdM"
      },
      "outputs": [],
      "source": [
        "import albumentations as A                              # for augmentation transform\n",
        "\n",
        "import numpy as np                                      # sci kit specials ;D\n",
        "import matplotlib.pyplot as plt                         # plots\n",
        "from PIL import Image                                   # for opening images as numpy arrays or torch tensors\n",
        "\n",
        "from datetime import datetime                           # for model timestamp\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset                    # preset class for creating a dataset\n",
        "from torch.utils.data import random_split               # for splitting datasets into training, test, validation\n",
        "from torch.utils.data import DataLoader                 # self-explanitory\n",
        "import segmentation_models_pytorch as smp               # preset model\n",
        "\n",
        "from tqdm import tqdm                                   # for the progress bar\n",
        "import os                                               # for accessing files and setting proper paths to   them\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter       # tensorboard srv\n",
        "from torch.nn.functional import binary_cross_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "XvxDfZEXuOdO"
      },
      "outputs": [],
      "source": [
        "DEBUG = False\n",
        "\n",
        "if DEBUG==True:\n",
        "\n",
        "  MASK_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_lab\"\n",
        "  IMG_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_img\"\n",
        "  MASK_TEST_PATH = \"../assets/datasets/DeepCrack/test_lab\"\n",
        "  IMG_TEST_PATH = \"../assets/datasets/DeepCrack/test_img\"\n",
        "  DEVICE = \"cpu\"\n",
        "  BATCH_SIZE = 2\n",
        "  WORKERS = 4\n",
        "\n",
        "else:\n",
        "  MASK_TRAIN_PATH = \"/content/DeepCrack/train_lab\"\n",
        "  IMG_TRAIN_PATH = \"/content/DeepCrack/train_img\"\n",
        "  MASK_TEST_PATH = \"/content/DeepCrack/test_lab\"\n",
        "  IMG_TEST_PATH = \"/content/DeepCrack/test_img\"\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "  DEVICE=\"cuda\"\n",
        "  WORKERS = 2\n",
        "\n",
        "PIN_MEMORY = True\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 40\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "\n",
        "SCHEDULER_PATIENCE = 5\n",
        "SCHEDULER_FACTOR = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "yshAUFtwuOdO"
      },
      "outputs": [],
      "source": [
        "def fetch_data(path) -> list:\n",
        "  \"\"\"Return files as their paths+filename in an array\"\"\"\n",
        "\n",
        "  assert (os.path.exists(path) == True),  \"Failure during data fetching\"\n",
        "\n",
        "  result = []\n",
        "  for file in tqdm(os.listdir(path), desc=f\"Loading files from {path} \",unit=\"File\", leave=True):\n",
        "    fpath = os.path.join(path,file)\n",
        "    result.append(fpath)\n",
        "\n",
        "  print(f\"{path} - len({len(result)})\")\n",
        "  return result\n",
        "\n",
        "\n",
        "class DeepCrackDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, transform=None):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    # sort values so the file names corespoding to each other are loaded in order\n",
        "    self.images = sorted([os.path.join(img_dir, file) for file in os.listdir(img_dir)] )\n",
        "    self.masks = sorted([os.path.join(mask_dir, file) for file in os.listdir(mask_dir)])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    np_image = np.array(Image.open(self.images[index]))\n",
        "    np_mask = np.array(Image.open(self.masks[index]))\n",
        "\n",
        "\n",
        "    if len(np_mask.shape) == 3:\n",
        "      np_mask = np_mask[:,:,0]\n",
        "\n",
        "    np_mask = (np_mask > 127).astype(np.uint8)\n",
        "\n",
        "    if self.transform: # if using transforms\n",
        "      t = self.transform(image=np_image, mask=np_mask)\n",
        "      np_image = t[\"image\"]\n",
        "      np_mask = t[\"mask\"]\n",
        "\n",
        "    # conversion from numpy array convention to tensor via permute,\n",
        "    #     then normalizing to [0,1] range, same for mask, only using binary data\n",
        "    tensor_image = torch.from_numpy(np_image).permute(2, 0, 1).float() / 255.0\n",
        "    tensor_mask = torch.from_numpy(np_mask).unsqueeze(0).float()\n",
        "\n",
        "    return tensor_image,tensor_mask\n",
        "\n",
        "\n",
        "def get_dataset(img_path, mask_path, transform_train = None ):\n",
        "\n",
        "  dataset = DeepCrackDataset(img_path, mask_path, transform=transform_train)\n",
        "  return dataset\n",
        "\n",
        "def split_dataset(dataset: DeepCrackDataset, test_factor:float, val_factor:float )->list:\n",
        "  \"\"\"Split exising dataset given percentages as [0,1] floats, return list of  \"\"\"\n",
        "  return random_split(dataset, [test_factor, val_factor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "W7UY0hh3uOdP"
      },
      "outputs": [],
      "source": [
        "transofrm_train = A.Compose([\n",
        "    A.Resize(512, 512),  # ← Stały rozmiar, bez crop\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.Rotate(limit=10, p=0.3),  # ← Mniejszy kąt\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),  # ← Mniej agresywne\n",
        "])\n",
        "\n",
        "transform_val = A.Compose([A.Resize(512,512)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "xrXVv0mLuOdQ"
      },
      "outputs": [],
      "source": [
        "# class DiceLoss(torch.nn.Module):\n",
        "#   def __init__(self, smooth=1e-6):\n",
        "#     super(DiceLoss,self).__init__()\n",
        "#     self.smooth = smooth\n",
        "#   def forward(self, predictions, targets):\n",
        "#     predictions = torch.sigmoid(predictions)\n",
        "\n",
        "#     predictions = predictions.view(-1)\n",
        "#     targets = targets.view(-1)\n",
        "\n",
        "#     intersection = (predictions * targets).sum()\n",
        "#     dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "#     return 1-dice\n",
        "\n",
        "\n",
        "# binary cross entropy\n",
        "\n",
        "class BCrossEntropyLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-6):\n",
        "    super(BCrossEntropyLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "    loss = binary_cross_entropy(predictions, targets)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "xOyfdE20uOdS"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions, targets, threshold=0.5):\n",
        "\n",
        "    # Binaryzacja\n",
        "    preds = (predictions > threshold).float()\n",
        "    targets = targets.float()\n",
        "\n",
        "    # Flatten\n",
        "    preds_flat = preds.view(-1)\n",
        "    targets_flat = targets.view(-1)\n",
        "\n",
        "    # True/False Positives/Negatives\n",
        "    TP = ((preds_flat == 1) & (targets_flat == 1)).sum().float()\n",
        "    TN = ((preds_flat == 0) & (targets_flat == 0)).sum().float()\n",
        "    FP = ((preds_flat == 1) & (targets_flat == 0)).sum().float()\n",
        "    FN = ((preds_flat == 0) & (targets_flat == 1)).sum().float()\n",
        "\n",
        "    conf_table = [[TP,FP],[FN, TN]]\n",
        "\n",
        "    # Metryki\n",
        "    epsilon = 1e-7  # Unikaj dzielenia przez zero\n",
        "\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "    specificity = TN / (TN + FP + epsilon)\n",
        "\n",
        "    # IoU (Intersection over Union) - NAJWAŻNIEJSZA dla segmentacji!\n",
        "    intersection = (preds * targets).sum()\n",
        "    union = preds.sum() + targets.sum() - intersection\n",
        "    iou = intersection / (union + epsilon)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * intersection) / (preds.sum() + targets.sum() + epsilon)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'f1_score': f1_score.item(),\n",
        "        'specificity': specificity.item(),\n",
        "        'iou': iou.item(),\n",
        "        'dice': dice.item(),\n",
        "        'confusion_table': conf_table\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "N3IY_qYBuOdT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = .0\n",
        "    metrics = {\n",
        "        'iou': [], 'dice': [], 'recall': [],\n",
        "        'precision': [], 'f1_score': []\n",
        "    }\n",
        "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        # move to adequete memory\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions_sigmoid = torch.sigmoid(predictions)\n",
        "            batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "            for key in metrics.keys():\n",
        "                value = batch_metrics[key]\n",
        "                if isinstance(value, torch.Tensor):\n",
        "                    value = value.cpu().item()\n",
        "                metrics[key].append(batch_metrics[key])\n",
        "\n",
        "        loop.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    loop.close()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "def validate(model, val_loader, criterion,device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  metrics = {\n",
        "        'iou': [], 'dice': [], 'recall': [],\n",
        "        'precision': [], 'f1_score': []\n",
        "    }\n",
        "  with torch.no_grad():\n",
        "    for images,masks in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "      images = images.to(device)\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      predictions = model(images)\n",
        "      loss = criterion(predictions, masks)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      predictions_sigmoid = torch.sigmoid(predictions)\n",
        "      batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "      for key in metrics.keys():\n",
        "          value = batch_metrics[key]\n",
        "\n",
        "          if isinstance(value, torch.Tensor):\n",
        "              value = value.cpu().item()\n",
        "\n",
        "          metrics[key].append(batch_metrics[key])\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "    return avg_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "MajrjmMsuOdU"
      },
      "outputs": [],
      "source": [
        "def main()-> int:\n",
        "\n",
        "    its_training_time = datetime.now().strftime('H%M') # Jared Leto likes it\n",
        "    writer = SummaryWriter(f\"runs/model_{its_training_time}\")\n",
        "\n",
        "    train_set = get_dataset(IMG_TRAIN_PATH, MASK_TRAIN_PATH, transform_train=transofrm_train)\n",
        "    testing_dataset = get_dataset(IMG_TEST_PATH, MASK_TEST_PATH, transform_train=transform_val)\n",
        "\n",
        "\n",
        "    test_set, val_set = split_dataset(testing_dataset, .5, .5 )\n",
        "\n",
        "    train_loader = DataLoader( train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader( test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader = DataLoader( val_set, batch_size=BATCH_SIZE , shuffle=False, num_workers=WORKERS    , pin_memory=PIN_MEMORY)\n",
        "\n",
        "    print(\"Datasets sizes: \")\n",
        "    print(f\"\\t  Train: {len(train_set)}\")\n",
        "    print(f\"\\t    Val: {len(val_set)}\")\n",
        "    print(f\"\\t   Test: {len(test_set)}\")\n",
        "\n",
        "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # criterion = DiceLoss()\n",
        "    criterion = BCrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',  # Maksymalizuj IoU\n",
        "        factor=SCHEDULER_FACTOR,\n",
        "        patience=SCHEDULER_PATIENCE,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining configuration:\")\n",
        "    print(f\"\\t       Optimizer: Adam\")\n",
        "    print(f\"\\t   Learning rate: {LEARNING_RATE}\")\n",
        "    print(f\"\\t    Weight decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"\\t       Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE})\")\n",
        "    print(f\"\\t  Early stopping: patience={EARLY_STOPPING_PATIENCE}\")\n",
        "    print(f\"\\t          Epochs: {EPOCHS}\")\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    best_val_iou = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    writer.add_text(\"Hparams\",f\"\"\"\n",
        "    -           Learning Rate: {LEARNING_RATE}\n",
        "    -              Batch Size: {BATCH_SIZE}\n",
        "    -            Weight Decay: {WEIGHT_DECAY}\n",
        "    -                  Epochs: {EPOCHS}\n",
        "    -      Scheduler Patience: {SCHEDULER_PATIENCE}\n",
        "    - Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\n",
        "    -                  Device: {DEVICE}\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch:{epoch}.\")\n",
        "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_metrics['iou'])\n",
        "\n",
        "        # Loss\n",
        "        writer.add_scalars('Loss', {\n",
        "            'train': train_metrics['loss'],\n",
        "            'val': val_metrics['loss']\n",
        "        }, epoch)\n",
        "\n",
        "        # IoU\n",
        "        writer.add_scalars('IoU', {\n",
        "            'train': train_metrics['iou'],\n",
        "            'val': val_metrics['iou']\n",
        "        }, epoch)\n",
        "\n",
        "        # Dice\n",
        "        writer.add_scalars('Dice', {\n",
        "            'train': train_metrics['dice'],\n",
        "            'val': val_metrics['dice']\n",
        "        }, epoch)\n",
        "\n",
        "        # Precision\n",
        "        writer.add_scalars('Precision', {\n",
        "            'train': train_metrics['precision'],\n",
        "            'val': val_metrics['precision']\n",
        "        }, epoch)\n",
        "\n",
        "        # Recall\n",
        "        writer.add_scalars('Recall', {\n",
        "            'train': train_metrics['recall'],\n",
        "            'val': val_metrics['recall']\n",
        "        }, epoch)\n",
        "\n",
        "        # F1-Score\n",
        "        writer.add_scalars('F1-Score', {\n",
        "            'train': train_metrics['f1_score'],\n",
        "            'val': val_metrics['f1_score']\n",
        "        }, epoch)\n",
        "\n",
        "        # Learning Rate\n",
        "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "\n",
        "        # # Accuracy (tylko validation)\n",
        "        # writer.add_scalar('Validation/Accuracy', val_metrics['accuracy'], epoch)\n",
        "\n",
        "        # SAVE BEST MODEL\n",
        "        if val_metrics['iou'] > best_val_iou:\n",
        "            best_val_iou = val_metrics['iou']\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_iou': best_val_iou,\n",
        "                'val_metrics': val_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, f'best_model_iou_{best_val_iou:.4f}.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # ========================================\n",
        "        # EARLY STOPPING\n",
        "        # ========================================\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n  Early stopping triggered!\")\n",
        "            print(f\"   No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "            print(f\"   Best IoU: {best_val_iou:.4f} at epoch {best_epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            val_images, val_masks = next(iter(val_loader))\n",
        "            val_images = val_images.to(device)\n",
        "            val_masks = val_masks.to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(val_images)\n",
        "                val_preds = torch.sigmoid(val_outputs)\n",
        "\n",
        "            # Weź pierwsze 4 obrazy\n",
        "            writer.add_images('Images/Input', val_images[:4], epoch)\n",
        "            writer.add_images('Images/Ground_Truth', val_masks[:4], epoch)\n",
        "            writer.add_images('Images/Prediction', val_preds[:4], epoch)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failure - Could not log images: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "    writer.close()\n",
        "    print(\"Training done.\")\n",
        "\n",
        "\n",
        "#     # Load best model\n",
        "#     checkpoint = torch.load(f'best_model_iou_{best_val_iou:.4f}.pth', weights_only=False)\n",
        "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "#     test_metrics = validate(model, test_loader, criterion, device)\n",
        "#      # Zapisz hyperparametry vs metryki (do porównania w TensorBoard)\n",
        "#     writer.add_hparams(\n",
        "#         {\n",
        "#             'lr': LEARNING_RATE,\n",
        "#             'batch_size': BATCH_SIZE,\n",
        "#             'weight_decay': WEIGHT_DECAY,\n",
        "#             'scheduler_patience': SCHEDULER_PATIENCE,\n",
        "#         },\n",
        "#         {\n",
        "#             'hparam/test_iou': test_metrics['iou'],\n",
        "#             'hparam/test_dice': test_metrics['dice'],\n",
        "#             'hparam/test_f1': test_metrics['f1_score'],\n",
        "#         }\n",
        "#     )\n",
        "\n",
        "#     writer.add_text('Final_Test_Metrics', f\"\"\"\n",
        "#     Test Set Results (Best Model from Epoch {best_epoch + 1})\n",
        "\n",
        "# -       IoU: {test_metrics['iou']:.4f}\n",
        "# -      Dice: {test_metrics['dice']:.4f}\n",
        "# -    Recall: {test_metrics['recall']:.4f}\n",
        "# - Precision: {test_metrics['precision']:.4f}\n",
        "# -  F1-Score: {test_metrics['f1_score']:.4f}\n",
        "# Best Validation IoU: {best_val_iou:.4f}\n",
        "#     \"\"\")\n",
        "\n",
        "#     # Dodaj finalne metryki jako skalary\n",
        "#     writer.add_scalar('Final/Test_IoU', test_metrics['iou'], 0)\n",
        "#     writer.add_scalar('Final/Test_Dice', test_metrics['dice'], 0)\n",
        "#     writer.add_scalar('Final/Test_Recall', test_metrics['recall'], 0)\n",
        "#     writer.add_scalar('Final/Test_Precision', test_metrics['precision'], 0)\n",
        "#     writer.add_scalar('Final/Test_F1', test_metrics['f1_score'], 0)\n",
        "\n",
        "\n",
        "\n",
        "#     print(f\"\\nTest Results:\")\n",
        "#     print(f\"   IoU:       {test_metrics['iou']:.4f}\")\n",
        "#     print(f\"   Dice:      {test_metrics['dice']:.4f}\")\n",
        "#     print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
        "#     print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
        "#     print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
        "\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKAh-oh7vQmJ",
        "outputId": "35b50ebb-7a80-4ac5-b9d4-89ebf58c673f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets sizes: \n",
            "\t  Train: 300\n",
            "\t    Val: 118\n",
            "\t   Test: 119\n",
            "\n",
            "Training configuration:\n",
            "\t       Optimizer: Adam\n",
            "\t   Learning rate: 0.0001\n",
            "\t    Weight decay: 1e-05\n",
            "\t       Scheduler: ReduceLROnPlateau (patience=5)\n",
            "\t  Early stopping: patience=15\n",
            "\t          Epochs: 40\n",
            "\n",
            "Epoch:0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:1.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:2.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:3.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:4.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:5.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:6.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:7.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:8.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:9.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:10.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:11.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:12.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:13.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:14.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:15.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:16.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:17.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:18.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:19.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:20.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:21.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:22.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:23.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:24.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:25.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:26.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:27.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:28.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:29.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:30.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:31.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:32.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:33.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:34.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:35.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:36.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:37.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:38.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch:39.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "ARRwxkmA0i52"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "sRrCX8OHk8-k",
        "outputId": "1c7563f6-5e07-4f6c-d81f-4ec03f4c4e1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "172"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAoqVXQL_tQc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_prediction(img:np.array, prediction, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Wizualizacja predykcji\n",
        "\n",
        "    Args:\n",
        "        image_path: ścieżka do oryginalnego obrazu\n",
        "        prediction: maska prawdopodobieństwa [H, W]\n",
        "        threshold: próg binaryzacji (default: 0.5)\n",
        "    \"\"\"\n",
        "    # Wczytaj oryginalny obraz\n",
        "    image = img\n",
        "    # np.array(Image.open(image_path))\n",
        "\n",
        "    # Binaryzacja predykcji\n",
        "    binary_mask = (prediction > threshold).astype(np.uint8)\n",
        "\n",
        "    # Wizualizacja\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # Oryginalny obraz\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=14)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Heatmapa prawdopodobieństwa\n",
        "    im1 = axes[1].imshow(prediction, cmap='hot', vmin=0, vmax=1)\n",
        "    axes[1].set_title(\"Probability Heatmap\", fontsize=14)\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
        "\n",
        "    # Binarna maska\n",
        "    axes[2].imshow(binary_mask, cmap='gray', vmin=0, vmax=1)\n",
        "    axes[2].set_title(f\"Binary Mask (threshold={threshold})\", fontsize=14)\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Overlay\n",
        "    axes[3].imshow(image)\n",
        "    axes[3].imshow(prediction, cmap='Reds', alpha=0.5, vmin=0, vmax=1)\n",
        "    axes[3].set_title(\"Overlay\", fontsize=14)\n",
        "    axes[3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statystyki\n",
        "    crack_pixels = binary_mask.sum()\n",
        "    total_pixels = binary_mask.size\n",
        "    crack_percentage = (crack_pixels / total_pixels) * 100\n",
        "\n",
        "    print(f\"📊 Statistics:\")\n",
        "    print(f\"   Crack pixels: {crack_pixels:,} ({crack_percentage:.2f}% of image)\")\n",
        "    print(f\"   Max probability: {prediction.max():.3f}\")\n",
        "    print(f\"   Mean probability: {prediction.mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9trnC4he_-ti",
        "outputId": "a3581135-1f73-45bb-c669-8a34cdd71481"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load(\"best_model_iou_0.5603.pth\", weights_only=False)\n",
        "\n",
        "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "# visualize_prediction(image_path, prediction, threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2iA-HYbAWkn"
      },
      "outputs": [],
      "source": [
        "testing_dataset = get_dataset(\"/content/DeepCrack/test_img\",\"/content/DeepCrack/test_lab\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1ghq4XWFudk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-C8SCkFGOpB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bachlor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}