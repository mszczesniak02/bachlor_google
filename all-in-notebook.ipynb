{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZXTX1HXuOdJ",
        "tags": [
          "command"
        ]
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mszczesniak02/bachlor_google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcFdDSJruOdK",
        "tags": [
          "command"
        ]
      },
      "outputs": [],
      "source": [
        "!cp -r /content/bachlor_google/DeepCrack/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3abHpmtZuOdL",
        "tags": [
          "command"
        ]
      },
      "outputs": [],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "--P1NscFuOdM"
      },
      "outputs": [],
      "source": [
        "import albumentations as A                              # for augmentation transform\n",
        "\n",
        "import numpy as np                                      # sci kit specials ;D\n",
        "import matplotlib.pyplot as plt                         # plots\n",
        "from PIL import Image                                   # for opening images as numpy arrays or torch tensors\n",
        "\n",
        "from datetime import datetime                           # for model timestamp\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset                    # preset class for creating a dataset\n",
        "from torch.utils.data import random_split               # for splitting datasets into training, test, validation\n",
        "from torch.utils.data import DataLoader                 # self-explanitory\n",
        "import segmentation_models_pytorch as smp               # preset model\n",
        "\n",
        "from tqdm import tqdm                                   # for the progress bar\n",
        "import os                                               # for accessing files and setting proper paths to   them\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter       # tensorboard srv\n",
        "from torch.nn.functional import binary_cross_entropy\n",
        "from torchvision.ops import sigmoid_focal_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XvxDfZEXuOdO"
      },
      "outputs": [],
      "source": [
        "DEBUG = False\n",
        "\n",
        "if DEBUG==True:\n",
        "\n",
        "  MASK_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_lab\"\n",
        "  IMG_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_img\"\n",
        "  MASK_TEST_PATH = \"../assets/datasets/DeepCrack/test_lab\"\n",
        "  IMG_TEST_PATH = \"../assets/datasets/DeepCrack/test_img\"\n",
        "  DEVICE = \"cuda\"\n",
        "  BATCH_SIZE = 2\n",
        "  WORKERS = 2\n",
        "\n",
        "else:\n",
        "  MASK_TRAIN_PATH = \"/content/DeepCrack/train_lab\"\n",
        "  IMG_TRAIN_PATH = \"/content/DeepCrack/train_img\"\n",
        "  MASK_TEST_PATH = \"/content/DeepCrack/test_lab\"\n",
        "  IMG_TEST_PATH = \"/content/DeepCrack/test_img\"\n",
        "  BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "  DEVICE=\"cuda\"\n",
        "  WORKERS = 2\n",
        "\n",
        "PIN_MEMORY = True\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 20\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "\n",
        "SCHEDULER_PATIENCE = 5\n",
        "SCHEDULER_FACTOR = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yshAUFtwuOdO"
      },
      "outputs": [],
      "source": [
        "class DeepCrackDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, transform=None):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    # sort values so the file names corespoding to each other are loaded in order\n",
        "    self.images = sorted([os.path.join(img_dir, file) for file in os.listdir(img_dir)] )\n",
        "    self.masks = sorted([os.path.join(mask_dir, file) for file in os.listdir(mask_dir)])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    np_image = np.array(Image.open(self.images[index]))\n",
        "    np_mask = np.array(Image.open(self.masks[index]))\n",
        "\n",
        "\n",
        "    if len(np_mask.shape) == 3:\n",
        "      np_mask = np_mask[:,:,0]\n",
        "\n",
        "    np_mask = (np_mask > 127).astype(np.uint8)\n",
        "\n",
        "    if self.transform: # if using transforms\n",
        "      t = self.transform(image=np_image, mask=np_mask)\n",
        "      np_image = t[\"image\"]\n",
        "      np_mask = t[\"mask\"]\n",
        "\n",
        "    # conversion from numpy array convention to tensor via permute,\n",
        "    #     then normalizing to [0,1] range, same for mask, only using binary data\n",
        "    tensor_image = torch.from_numpy(np_image).permute(2, 0, 1).float() / 255.0\n",
        "    tensor_mask = torch.from_numpy(np_mask).unsqueeze(0).float()\n",
        "\n",
        "    return tensor_image,tensor_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "W7UY0hh3uOdP"
      },
      "outputs": [],
      "source": [
        "transform_trainn = A.Compose([\n",
        "    A.Resize(512, 512),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.Rotate(limit=10, p=0.3),\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),\n",
        "])\n",
        "\n",
        "transform_val = A.Compose([A.Resize(512,512)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xrXVv0mLuOdQ"
      },
      "outputs": [],
      "source": [
        "class BCrossEntropyLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-16): # zmiana\n",
        "    super(BCrossEntropyLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "    loss = binary_cross_entropy(predictions, targets)\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dnsNycrDgFIn"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-16): # zmiana\n",
        "    super(FocalLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    loss =sigmoid_focal_loss(predictions,targets, alpha=.25, gamma=2.0).mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IBfUBI_zgFIo"
      },
      "outputs": [],
      "source": [
        "class BCE_with_FocalLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-16): # zmiana\n",
        "    super(BCE_with_FocalLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    # loss_focal = sigmoid_focal_loss(predictions,targets)\n",
        "    # loss_bce = binary_cross_entropy(predictions, targets)\n",
        "    loss = 0.5 * sigmoid_focal_loss(predictions,targets, alpha=.25, gamma=2.0).mean()+ 0.5 * binary_cross_entropy(torch.sigmoid(predictions), targets)\n",
        "\n",
        "    return loss.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fW6dPHyagFIo"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-6):\n",
        "    super(DiceLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "\n",
        "    predictions = predictions.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (predictions * targets).sum()\n",
        "    dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "    return 1-dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Co7RFkMWgFIp"
      },
      "outputs": [],
      "source": [
        "class DiceFocalLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-6):\n",
        "    super(DiceFocalLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "\n",
        "    focal_loss = sigmoid_focal_loss(predictions,targets, alpha=.25, gamma=2.0).mean()\n",
        "\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "    predictions = predictions.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (predictions * targets).sum()\n",
        "    dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "    dice_loss = 1-dice\n",
        "\n",
        "    the_loss = dice_loss * 0.5 + focal_loss * 0.5\n",
        "\n",
        "    return the_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xOyfdE20uOdS"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions, targets, threshold=0.5):\n",
        "\n",
        "    # Binaryzacja\n",
        "    preds = (predictions > threshold).float()\n",
        "    targets = targets.float()\n",
        "\n",
        "    # Flatten\n",
        "    preds_flat = preds.view(-1)\n",
        "    targets_flat = targets.view(-1)\n",
        "\n",
        "    # True/False Positives/Negatives\n",
        "    TP = ((preds_flat == 1) & (targets_flat == 1)).sum().float()\n",
        "    TN = ((preds_flat == 0) & (targets_flat == 0)).sum().float()\n",
        "    FP = ((preds_flat == 1) & (targets_flat == 0)).sum().float()\n",
        "    FN = ((preds_flat == 0) & (targets_flat == 1)).sum().float()\n",
        "\n",
        "    conf_table = [[TP,FP],[FN, TN]]\n",
        "\n",
        "    # Metryki\n",
        "    epsilon = 1e-7  # Unikaj dzielenia przez zero\n",
        "\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "    specificity = TN / (TN + FP + epsilon)\n",
        "\n",
        "    # IoU (Intersection over Union) - NAJWAŻNIEJSZA dla segmentacji!\n",
        "    intersection = (preds * targets).sum()\n",
        "    union = preds.sum() + targets.sum() - intersection\n",
        "    iou = intersection / (union + epsilon)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * intersection) / (preds.sum() + targets.sum() + epsilon)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'f1_score': f1_score.item(),\n",
        "        'specificity': specificity.item(),\n",
        "        'iou': iou.item(),\n",
        "        'dice': dice.item(),\n",
        "        'confusion_table': conf_table\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "N3IY_qYBuOdT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = .0\n",
        "    metrics = {\n",
        "        'iou': [], 'dice': [], 'recall': [],\n",
        "        'precision': [], 'f1_score': []\n",
        "    }\n",
        "\n",
        "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "        # Forward pass\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, masks)\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions_sigmoid = torch.sigmoid(predictions)\n",
        "            batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "            for key in metrics.keys():\n",
        "                value = batch_metrics[key]\n",
        "                if isinstance(value, torch.Tensor):\n",
        "                    value = value.cpu().item()\n",
        "                metrics[key].append(batch_metrics[key])\n",
        "\n",
        "        loop.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    loop.close()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "def validate(model, val_loader, criterion,device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  metrics = {\n",
        "        'iou': [], 'dice': [], 'recall': [],\n",
        "        'precision': [], 'f1_score': []\n",
        "    }\n",
        "  with torch.no_grad():\n",
        "    for images,masks in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "\n",
        "      images = images.to(device)\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      predictions = model(images)\n",
        "      loss = criterion(predictions, masks)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      predictions_sigmoid = torch.sigmoid(predictions)\n",
        "      batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "      for key in metrics.keys():\n",
        "          value = batch_metrics[key]\n",
        "\n",
        "          if isinstance(value, torch.Tensor):\n",
        "              value = value.cpu().item()\n",
        "\n",
        "          metrics[key].append(batch_metrics[key])\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "    return avg_metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "MajrjmMsuOdU"
      },
      "outputs": [],
      "source": [
        "def main()-> int:\n",
        "\n",
        "    its_training_time = datetime.now().strftime('H%M') # Jared Leto likes it\n",
        "    writer = SummaryWriter(f\"runs/model_{its_training_time}\")\n",
        "\n",
        "    train_set = DeepCrackDataset(IMG_TRAIN_PATH, MASK_TRAIN_PATH, transform=transform_trainn)\n",
        "    val_dataset = DeepCrackDataset(IMG_TEST_PATH, MASK_TEST_PATH, transform=transform_val)\n",
        "\n",
        "    test_set, val_set =  random_split(val_dataset, [.2, .8])\n",
        "\n",
        "    train_loader    = DataLoader( train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader     = DataLoader( test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader      = DataLoader( val_set, batch_size=BATCH_SIZE , shuffle=False, num_workers=WORKERS    , pin_memory=PIN_MEMORY)\n",
        "\n",
        "    print(\"Datasets sizes: \")\n",
        "    print(f\"\\t  Train: {len(train_set)}\")\n",
        "    print(f\"\\t    Val: {len(val_set)}\")\n",
        "    print(f\"\\t   Test: {len(test_set)}\")\n",
        "\n",
        "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    criterion = BCrossEntropyLoss()\n",
        "\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',  # Maksymalizuj IoU\n",
        "        factor=SCHEDULER_FACTOR,\n",
        "        patience=SCHEDULER_PATIENCE,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining configuration:\")\n",
        "    print(f\"\\t       Optimizer: Adam\")\n",
        "    print(f\"\\t   Learning rate: {LEARNING_RATE}\")\n",
        "    print(f\"\\t    Weight decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"\\t       Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE})\")\n",
        "    print(f\"\\t  Early stopping: patience={EARLY_STOPPING_PATIENCE}\")\n",
        "    print(f\"\\t          Epochs: {EPOCHS}\")\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    best_val_iou = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    writer.add_text(\"Hparams\",f\"\"\"\n",
        "    -           Learning Rate: {LEARNING_RATE}\n",
        "    -              Batch Size: {BATCH_SIZE}\n",
        "    -            Weight Decay: {WEIGHT_DECAY}\n",
        "    -                  Epochs: {EPOCHS}\n",
        "    -      Scheduler Patience: {SCHEDULER_PATIENCE}\n",
        "    - Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\n",
        "    -                  Device: {DEVICE}\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Epochs:{epochs}, current:\",end='')\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"{epoch},\")\n",
        "\n",
        "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_metrics['iou'])\n",
        "\n",
        "        # Loss\n",
        "        writer.add_scalars('Loss', {\n",
        "            'train': train_metrics['loss'],\n",
        "            'val': val_metrics['loss']\n",
        "        }, epoch)\n",
        "\n",
        "        # IoU\n",
        "        writer.add_scalars('IoU', {\n",
        "            'train': train_metrics['iou'],\n",
        "            'val': val_metrics['iou']\n",
        "        }, epoch)\n",
        "\n",
        "        # Dice\n",
        "        writer.add_scalars('Dice', {\n",
        "            'train': train_metrics['dice'],\n",
        "            'val': val_metrics['dice']\n",
        "        }, epoch)\n",
        "\n",
        "        # Precision\n",
        "        writer.add_scalars('Precision', {\n",
        "            'train': train_metrics['precision'],\n",
        "            'val': val_metrics['precision']\n",
        "        }, epoch)\n",
        "\n",
        "        # Recall\n",
        "        writer.add_scalars('Recall', {\n",
        "            'train': train_metrics['recall'],\n",
        "            'val': val_metrics['recall']\n",
        "        }, epoch)\n",
        "\n",
        "        # F1-Score\n",
        "        writer.add_scalars('F1-Score', {\n",
        "            'train': train_metrics['f1_score'],\n",
        "            'val': val_metrics['f1_score']\n",
        "        }, epoch)\n",
        "\n",
        "        # Learning Rate\n",
        "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "\n",
        "        # # Accuracy (tylko validation)\n",
        "        # writer.add_scalar('Validation/Accuracy', val_metrics['accuracy'], epoch)\n",
        "\n",
        "        # SAVE BEST MODEL\n",
        "        if val_metrics['iou'] > best_val_iou:\n",
        "            best_val_iou = val_metrics['iou']\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_iou': best_val_iou,\n",
        "                'val_metrics': val_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, f'best_model_iou_{best_val_iou:.4f}.pth')\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # ========================================\n",
        "        # EARLY STOPPING\n",
        "        # ========================================\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n  Early stopping triggered!\")\n",
        "            print(f\"   No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "            print(f\"   Best IoU: {best_val_iou:.4f} at epoch {best_epoch + 1}\")\n",
        "            break\n",
        "\n",
        "\n",
        "        val_images, val_masks = next(iter(val_loader))\n",
        "        val_images = val_images.to(device)\n",
        "        val_masks = val_masks.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(val_images)\n",
        "            val_preds = torch.sigmoid(val_outputs)\n",
        "\n",
        "        # Weź pierwsze 4 obrazy\n",
        "        writer.add_images('Images/Input', val_images[:4], epoch)\n",
        "        writer.add_images('Images/Ground_Truth', val_masks[:4], epoch)\n",
        "        writer.add_images('Images/Prediction', val_preds[:4], epoch)\n",
        "\n",
        "\n",
        "    writer.close()\n",
        "    print(\"Training done.\")\n",
        "\n",
        "\n",
        "#     # Load best model\n",
        "    checkpoint = torch.load(f'best_model_iou_{best_val_iou:.4f}.pth', weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    test_metrics = validate(model, test_loader, criterion, device)\n",
        "#      # Zapisz hyperparametry vs metryki (do porównania w TensorBoard)\n",
        "#     writer.add_hparams(\n",
        "#         {\n",
        "#             'lr': LEARNING_RATE,\n",
        "#             'batch_size': BATCH_SIZE,\n",
        "#             'weight_decay': WEIGHT_DECAY,\n",
        "#             'scheduler_patience': SCHEDULER_PATIENCE,\n",
        "#         },\n",
        "#         {\n",
        "#             'hparam/test_iou': test_metrics['iou'],\n",
        "#             'hparam/test_dice': test_metrics['dice'],\n",
        "#             'hparam/test_f1': test_metrics['f1_score'],\n",
        "#         }\n",
        "#     )\n",
        "\n",
        "#     writer.add_text('Final_Test_Metrics', f\"\"\"\n",
        "    # Test Set Results (Best Model from Epoch {best_epoch + 1})\n",
        "\n",
        "# -       IoU: {test_metrics['iou']:.4f}\n",
        "# -      Dice: {test_metrics['dice']:.4f}\n",
        "# -    Recall: {test_metrics['recall']:.4f}\n",
        "# - Precision: {test_metrics['precision']:.4f}\n",
        "# -  F1-Score: {test_metrics['f1_score']:.4f}\n",
        "# Best Validation IoU: {best_val_iou:.4f}\n",
        "#     \"\"\")\n",
        "\n",
        "#     # Dodaj finalne metryki jako skalary\n",
        "#     writer.add_scalar('Final/Test_IoU', test_metrics['iou'], 0)\n",
        "#     writer.add_scalar('Final/Test_Dice', test_metrics['dice'], 0)\n",
        "#     writer.add_scalar('Final/Test_Recall', test_metrics['recall'], 0)\n",
        "#     writer.add_scalar('Final/Test_Precision', test_metrics['precision'], 0)\n",
        "#     writer.add_scalar('Final/Test_F1', test_metrics['f1_score'], 0)\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"   IoU:       {test_metrics['iou']:.4f}\")\n",
        "    print(f\"   Dice:      {test_metrics['dice']:.4f}\")\n",
        "    print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
        "    print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
        "\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKAh-oh7vQmJ",
        "outputId": "62ea2dbb-de97-405a-c5ef-5a95c50ebbfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets sizes: \n",
            "\t  Train: 899\n",
            "\t    Val: 349\n",
            "\t   Test: 88\n",
            "\n",
            "Training configuration:\n",
            "\t       Optimizer: Adam\n",
            "\t   Learning rate: 0.0001\n",
            "\t    Weight decay: 1e-05\n",
            "\t       Scheduler: ReduceLROnPlateau (patience=5)\n",
            "\t  Early stopping: patience=15\n",
            "\t          Epochs: 20\n",
            "Epochs:20, current:0,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                         "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "   IoU:       0.6750\n",
            "   Dice:      0.8051\n",
            "   Recall:    0.7475\n",
            "   Precision: 0.8761\n",
            "   F1-Score:  0.8051\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ARRwxkmA0i52"
      },
      "outputs": [],
      "source": [
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sRrCX8OHk8-k",
        "outputId": "5024009f-2a10-431a-9233-2922bfe4abe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory allocated before clearing cache: 298679808 bytes\n",
            "Memory allocated after clearing cache: 0 bytes\n"
          ]
        }
      ],
      "source": [
        "print(f\"Memory allocated before clearing cache: {torch.cuda.memory_allocated()} bytes\")\n",
        "\n",
        "# Manually invoke garbage collection\n",
        "\n",
        "# Clear GPU cache\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "print(f\"Memory allocated after clearing cache: {torch.cuda.memory_allocated()} bytes\")\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "vKxVDB-fgFIt"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9WhZfDMMgFIu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bachlor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}