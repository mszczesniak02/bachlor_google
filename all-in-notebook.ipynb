{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": [
          "command"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZXTX1HXuOdJ",
        "outputId": "4c0b74de-819d-4547-d5e0-4f5e1569817a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'bachlor_google'...\n",
            "remote: Enumerating objects: 1112, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 1112 (delta 1), reused 5 (delta 1), pack-reused 1106 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1112/1112), 66.90 MiB | 14.71 MiB/s, done.\n",
            "Resolving deltas: 100% (238/238), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mszczesniak02/bachlor_google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "tags": [
          "command"
        ],
        "id": "pcFdDSJruOdK"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/bachlor_google/DeepCrack/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "tags": [
          "command"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3abHpmtZuOdL",
        "outputId": "975c03f2-1de2-4b62-dc17-87eebc0d60a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.20)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.5)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "--P1NscFuOdM"
      },
      "outputs": [],
      "source": [
        "import albumentations as A                              # for augmentation transform\n",
        "\n",
        "import numpy as np                                      # sci kit specials ;D\n",
        "import matplotlib.pyplot as plt                         # plots\n",
        "from PIL import Image                                   # for opening images as numpy arrays or torch tensors\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset                    # preset class for creating a dataset\n",
        "from torch.utils.data import random_split               # for splitting datasets into training, test, validation\n",
        "from torch.utils.data import DataLoader                 # self-explanitory\n",
        "import segmentation_models_pytorch as smp               # preset model\n",
        "\n",
        "from tqdm import tqdm                                   # for the progress bar\n",
        "import os                                               # for accessing files and setting proper paths to   them\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter       # tensorboard srv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "XvxDfZEXuOdO"
      },
      "outputs": [],
      "source": [
        "DEBUG = False\n",
        "\n",
        "if DEBUG==True:\n",
        "\n",
        "  MASK_PATH = \"../assets/datasets/DeepCrack/train_lab\"\n",
        "  IMAGE_PATH = \"../assets/datasets/DeepCrack/train_img\"\n",
        "  DEVICE = \"cpu\"\n",
        "  WORKERS = 4\n",
        "\n",
        "else:\n",
        "  MASK_PATH = \"/content/DeepCrack/train_lab\"\n",
        "  IMAGE_PATH = \"/content/DeepCrack/train_img\"\n",
        "  DEVICE=\"cuda\"\n",
        "  WORKERS = 4\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "PIN_MEMORY = True\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 10\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "\n",
        "SCHEDULER_PATIENCE = 5\n",
        "SCHEDULER_FACTOR = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "yshAUFtwuOdO"
      },
      "outputs": [],
      "source": [
        "def fetch_data(path) -> list:\n",
        "  \"\"\"Return files as their paths+filename in an array\"\"\"\n",
        "\n",
        "  assert (os.path.exists(path) == True),  \"Failure during data fetching\"\n",
        "\n",
        "  result = []\n",
        "  for file in tqdm(os.listdir(path), desc=f\"Loading files from {path} \",unit=\"File\", leave=True):\n",
        "    fpath = os.path.join(path,file)\n",
        "    result.append(fpath)\n",
        "\n",
        "  print(f\"{path} - len({len(result)})\")\n",
        "  return result\n",
        "\n",
        "\n",
        "class DeepCrackDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, transform=None):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    # sort values so the file names corespoding to each other are loaded in order\n",
        "    self.images = sorted([os.path.join(img_dir, file) for file in os.listdir(img_dir)] )\n",
        "    self.masks = sorted([os.path.join(mask_dir, file) for file in os.listdir(mask_dir)])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    np_image = np.array(Image.open(self.images[index]))\n",
        "    np_mask = np.array(Image.open(self.masks[index]))\n",
        "\n",
        "\n",
        "    if len(np_mask.shape) == 3:\n",
        "      np_mask = np_mask[:,:,0]\n",
        "\n",
        "    np_mask = (np_mask > 127).astype(np.uint8)\n",
        "\n",
        "    if self.transform: # if using transforms\n",
        "      t = self.transform(image=np_image, mask=np_mask)\n",
        "      np_image = t[\"image\"]\n",
        "      np_mask = t[\"mask\"]\n",
        "\n",
        "    # conversion from numpy array convention to tensor via permute,\n",
        "    #     then normalizing to [0,1] range, same for mask, only using binary data\n",
        "    tensor_image = torch.from_numpy(np_image).permute(2, 0, 1).float() / 255.0\n",
        "    tensor_mask = torch.from_numpy(np_mask).unsqueeze(0).float()\n",
        "\n",
        "    return tensor_image,tensor_mask\n",
        "\n",
        "\n",
        "def get_dataset(img_path = IMAGE_PATH, mask_path = MASK_PATH ):\n",
        "\n",
        "  dataset = DeepCrackDataset(img_path, mask_path, transform=transofrm_train)\n",
        "  return dataset\n",
        "\n",
        "def split_dataset(dataset: DeepCrackDataset, train_factor, test_factor, val_factor )->list:\n",
        "  \"\"\"Split exising dataset given percentages as [0,1] floats, return list of  \"\"\"\n",
        "  train_set_len, test_set_len, val_set_len = int(dataset.__len__() * train_factor), int(dataset.__len__() * test_factor) , int(dataset.__len__() * val_factor)\n",
        "  train_set, test_set ,val_set = random_split(dataset, [train_set_len, test_set_len, val_set_len])\n",
        "\n",
        "  return [train_set, test_set, val_set]\n",
        "\n",
        "def show_dataset(data_loader, samples=4):\n",
        "    counter = 0\n",
        "    for images, masks in data_loader:\n",
        "        fig, axes = plt.subplots(samples, 2, figsize=(8, 12))\n",
        "        for i in range( samples ):\n",
        "\n",
        "            img = images[i].permute(1, 2, 0).numpy()\n",
        "            axes[i, 0].imshow(img)\n",
        "            axes[i, 0].set_title(f\"Image {i+1}\")\n",
        "            axes[i, 0].axis('off')\n",
        "\n",
        "            # # Maska\n",
        "            mask = masks[i, 0].numpy()\n",
        "            axes[i, 1].imshow(mask, cmap='gray')\n",
        "            axes[i, 1].set_title(f\"Mask {i+1}\")\n",
        "            axes[i, 1].axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        counter+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "W7UY0hh3uOdP"
      },
      "outputs": [],
      "source": [
        "transofrm_train = A.Compose([\n",
        "    A.Resize(512, 512),  # ← Stały rozmiar, bez crop\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.Rotate(limit=10, p=0.3),  # ← Mniejszy kąt\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),  # ← Mniej agresywne\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "xrXVv0mLuOdQ"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-6):\n",
        "    super(DiceLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "\n",
        "    predictions = predictions.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (predictions * targets).sum()\n",
        "    dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "    return 1-dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xOyfdE20uOdS"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions, targets, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Oblicz metryki segmentacji dla pojedynczego batcha\n",
        "\n",
        "    Args:\n",
        "        predictions: tensor [B, 1, H, W] - output z modelu (po sigmoid)\n",
        "        targets: tensor [B, 1, H, W] - ground truth maski\n",
        "        threshold: próg binaryzacji (default 0.5)\n",
        "\n",
        "    Returns:\n",
        "        dict z metrykami\n",
        "    \"\"\"\n",
        "    # Binaryzacja\n",
        "    preds = (predictions > threshold).float()\n",
        "    targets = targets.float()\n",
        "\n",
        "    # Flatten\n",
        "    preds_flat = preds.view(-1)\n",
        "    targets_flat = targets.view(-1)\n",
        "\n",
        "    # True/False Positives/Negatives\n",
        "    TP = ((preds_flat == 1) & (targets_flat == 1)).sum().float()\n",
        "    TN = ((preds_flat == 0) & (targets_flat == 0)).sum().float()\n",
        "    FP = ((preds_flat == 1) & (targets_flat == 0)).sum().float()\n",
        "    FN = ((preds_flat == 0) & (targets_flat == 1)).sum().float()\n",
        "\n",
        "    # Metryki\n",
        "    epsilon = 1e-7  # Unikaj dzielenia przez zero\n",
        "\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "    specificity = TN / (TN + FP + epsilon)\n",
        "\n",
        "    # IoU (Intersection over Union) - NAJWAŻNIEJSZA dla segmentacji!\n",
        "    intersection = (preds * targets).sum()\n",
        "    union = preds.sum() + targets.sum() - intersection\n",
        "    iou = intersection / (union + epsilon)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * intersection) / (preds.sum() + targets.sum() + epsilon)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'f1_score': f1_score.item(),\n",
        "        'specificity': specificity.item(),\n",
        "        'iou': iou.item(),\n",
        "        'dice': dice.item(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "N3IY_qYBuOdT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = .0\n",
        "    metrics = {\n",
        "        'iou': [], 'dice': [], 'recall': [],\n",
        "        'precision': [], 'f1_score': []\n",
        "    }\n",
        "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        # move to adequete memory\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions_sigmoid = torch.sigmoid(predictions)\n",
        "            batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "            for key in metrics.keys():\n",
        "                metrics[key].append(batch_metrics[key])\n",
        "\n",
        "        loop.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    loop.close()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "def validate(model, val_loader, criterion,device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  metrics = {\n",
        "      'iou': [], 'dice': [], 'recall': [],\n",
        "      'precision': [], 'f1_score': [], 'accuracy': []\n",
        "      }\n",
        "  with torch.no_grad():\n",
        "    for images,masks in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "      images = images.to(device)\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      predictions = model(images)\n",
        "      loss = criterion(predictions, masks)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      predictions_sigmoid = torch.sigmoid(predictions)\n",
        "      batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "      for key in metrics.keys():\n",
        "          metrics[key].append(batch_metrics[key])\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "\n",
        "\n",
        "def make_checkpoint(model, optimizer, best_val_loss, training_loss):\n",
        "   torch.save({\n",
        "      'epoch': EPOCHS,\n",
        "      'model_state_dict': model.state_dict(),\n",
        "      'optimizer_state_dict': optimizer.state_dict(),\n",
        "      'train_loss': training_loss,\n",
        "      'val_loss': best_val_loss,} , 'unet_MODEL_save.pth')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Kompletna ewaluacja modelu\n",
        "\n",
        "    Returns:\n",
        "        metrics: dict z metrykami\n",
        "        predictions: array predykcji [N, H, W]\n",
        "        ground_truths: array prawdziwych masek [N, H, W]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "\n",
        "    print(\"Running evaluation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            predictions = torch.sigmoid(outputs)\n",
        "\n",
        "            # Do CPU\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_ground_truths.append(masks.cpu().numpy())\n",
        "\n",
        "    # Concatenate\n",
        "    predictions = np.concatenate(all_predictions, axis=0)[:, 0]  # [N, H, W]\n",
        "    ground_truths = np.concatenate(all_ground_truths, axis=0)[:, 0]  # [N, H, W]\n",
        "\n",
        "    # Binaryzacja\n",
        "    pred_binary = (predictions > threshold).astype(np.float32)\n",
        "    gt_binary = (ground_truths > 0.5).astype(np.float32)\n",
        "\n",
        "    # ========================================\n",
        "    # OBLICZ METRYKI\n",
        "    # ========================================\n",
        "    ious = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "\n",
        "    for pred, gt in zip(pred_binary, gt_binary):\n",
        "        # Confusion matrix\n",
        "        tp = (pred * gt).sum()\n",
        "        fp = (pred * (1 - gt)).sum()\n",
        "        fn = ((1 - pred) * gt).sum()\n",
        "        tn = ((1 - pred) * (1 - gt)).sum()\n",
        "\n",
        "        # IoU (Intersection over Union)\n",
        "        iou = tp / (tp + fp + fn + 1e-6)\n",
        "        ious.append(iou)\n",
        "\n",
        "        # Precision (jakość detekcji)\n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        precisions.append(precision)\n",
        "\n",
        "        # Recall (czułość)\n",
        "        recall = tp / (tp + fn + 1e-6)\n",
        "        recalls.append(recall)\n",
        "\n",
        "        # F1 Score (harmonic mean)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    metrics = {\n",
        "        'iou_mean': np.mean(ious),\n",
        "        'iou_std': np.std(ious),\n",
        "        'iou_median': np.median(ious),\n",
        "        'precision_mean': np.mean(precisions),\n",
        "        'precision_std': np.std(precisions),\n",
        "        'recall_mean': np.mean(recalls),\n",
        "        'recall_std': np.std(recalls),\n",
        "        'f1_mean': np.mean(f1_scores),\n",
        "        'f1_std': np.std(f1_scores),\n",
        "        'accuracy_mean': np.mean(accuracies),\n",
        "        'ious': ious,\n",
        "        'precisions': precisions,\n",
        "        'recalls': recalls,\n",
        "        'f1_scores': f1_scores,\n",
        "    }\n",
        "\n",
        "    return metrics, predictions, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "MajrjmMsuOdU"
      },
      "outputs": [],
      "source": [
        "def main()-> int:\n",
        "\n",
        "    training_dataset = get_dataset(IMAGE_PATH, MASK_PATH)\n",
        "    testing_dataset = get_dataset(\"/content/DeepCrack/test_img\",\"/content/DeepCrack/test_lab\" )\n",
        "\n",
        "    # [train_set, test_set, val_set] = split_dataset(dataset, .8, .1, .1)\n",
        "\n",
        "    train_set = training_dataset\n",
        "    test_set, val_set = random_split(training_dataset, [.5, .5])\n",
        "\n",
        "\n",
        "    # [test_set, val_set] = split_dataset()\n",
        "\n",
        "    train_loader = DataLoader( train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader( test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader = DataLoader( val_set, batch_size=BATCH_SIZE , shuffle=False, num_workers=WORKERS    , pin_memory=PIN_MEMORY)\n",
        "\n",
        "    print(\"Loading datasets...\")\n",
        "    print(f\"   Train: {len(train_set)} images\")\n",
        "    print(f\"   Val:   {len(val_set)} images\")\n",
        "    print(f\"   Test:  {len(test_set)} images\")\n",
        "\n",
        "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"   Total parameters: {total_params:,}\")\n",
        "    print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "    print(f\"   Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
        "\n",
        "\n",
        "\n",
        "    criterion = DiceLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',  # Maksymalizuj IoU\n",
        "        factor=SCHEDULER_FACTOR,\n",
        "        patience=SCHEDULER_PATIENCE,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining configuration:\")\n",
        "    print(f\"   Optimizer: Adam\")\n",
        "    print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "    print(f\"   Weight decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"   Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE})\")\n",
        "    print(f\"   Early stopping: patience={EARLY_STOPPING_PATIENCE}\")\n",
        "    print(f\"   Epochs: {EPOCHS}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    best_val_iou = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        print(f\"\\n{'=' * 80}\")\n",
        "        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
        "        print(f\"{'=' * 80}\")\n",
        "\n",
        "        # TRAIN\n",
        "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "\n",
        "        # VALIDATE\n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        # SCHEDULER\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_metrics['iou'])\n",
        "\n",
        "\n",
        "\n",
        "       # ========================================\n",
        "        # PRINT METRICS\n",
        "        # ========================================\n",
        "        print(f\"\\nTraining:\")\n",
        "        print(f\"   Loss: {train_metrics['loss']:.4f}\")\n",
        "        print(f\"   IoU:  {train_metrics['iou']:.4f}\")\n",
        "        print(f\"   Dice: {train_metrics['dice']:.4f}\")\n",
        "\n",
        "        print(f\"\\nValidation:\")\n",
        "        print(f\"   Loss:      {val_metrics['loss']:.4f}\")\n",
        "        print(f\"   IoU:       {val_metrics['iou']:.4f} {'✅ NEW BEST!' if val_metrics['iou'] > best_val_iou else ''}\")\n",
        "        print(f\"   Dice:      {val_metrics['dice']:.4f}\")\n",
        "        print(f\"   Recall:    {val_metrics['recall']:.4f}\")\n",
        "        print(f\"   Precision: {val_metrics['precision']:.4f}\")\n",
        "        print(f\"   F1-Score:  {val_metrics['f1_score']:.4f}\")\n",
        "\n",
        "        print(f\"\\n LR: {current_lr:.6f}\")\n",
        "\n",
        "        # ========================================\n",
        "        # SAVE BEST MODEL\n",
        "        # ========================================\n",
        "        if val_metrics['iou'] > best_val_iou:\n",
        "            best_val_iou = val_metrics['iou']\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_iou': best_val_iou,\n",
        "                'val_metrics': val_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, f'best_model_iou_{best_val_iou:.4f}.pth')\n",
        "            print(f\"\\n Model saved: best_model_iou_{best_val_iou:.4f}.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"\\n No improvement for {patience_counter} epoch(s)\")\n",
        "\n",
        "        # ========================================\n",
        "        # EARLY STOPPING\n",
        "        # ========================================\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n  Early stopping triggered!\")\n",
        "            print(f\"   No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "            print(f\"   Best IoU: {best_val_iou:.4f} at epoch {best_epoch + 1}\")\n",
        "            break\n",
        "\n",
        "    # ========================================\n",
        "    # FINAL EVALUATION ON TEST SET\n",
        "    # ========================================\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"Final evaluation on test set...\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f'best_model_iou_{best_val_iou:.4f}.pth', weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    test_metrics = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"   IoU:       {test_metrics['iou']:.4f}\")\n",
        "    print(f\"   Dice:      {test_metrics['dice']:.4f}\")\n",
        "    print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
        "    print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
        "    print(f\"   Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
        "\n",
        "    # Save to TensorBoard\n",
        "    # writer.add_text('Final_Test_Metrics', str(test_metrics))\n",
        "\n",
        "    # writer.close()\n",
        "\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\" TRAINING COMPLETED!\")\n",
        "    print(f\"   Best Validation IoU: {best_val_iou:.4f} (epoch {best_epoch + 1})\")\n",
        "    print(f\"   Test IoU: {test_metrics['iou']:.4f}\")\n",
        "    print(f\"   Model saved as: best_model_iou_{best_val_iou:.4f}.pth\")\n",
        "    print(f\"{'=' * 80}\\n\")\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKAh-oh7vQmJ",
        "outputId": "127b9131-5282-4db9-fc62-c26c48007e25"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets...\n",
            "   Train: 300 images\n",
            "   Val:   150 images\n",
            "   Test:  150 images\n",
            "   Total parameters: 24,436,369\n",
            "   Trainable parameters: 24,436,369\n",
            "   Model size: ~97.7 MB\n",
            "\n",
            "Training configuration:\n",
            "   Optimizer: Adam\n",
            "   Learning rate: 0.0001\n",
            "   Weight decay: 1e-05\n",
            "   Scheduler: ReduceLROnPlateau (patience=5)\n",
            "   Early stopping: patience=15\n",
            "   Epochs: 10\n",
            "\n",
            "================================================================================\n",
            "Epoch 1/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.9163\n",
            "   IoU:  0.0382\n",
            "   Dice: 0.0734\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.9061\n",
            "   IoU:       0.0480 ✅ NEW BEST!\n",
            "   Dice:      0.0914\n",
            "   Recall:    0.9783\n",
            "   Precision: 0.0480\n",
            "   F1-Score:  0.0914\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.0480.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 2/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.8954\n",
            "   IoU:  0.0628\n",
            "   Dice: 0.1175\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.8857\n",
            "   IoU:       0.0775 ✅ NEW BEST!\n",
            "   Dice:      0.1435\n",
            "   Recall:    0.9978\n",
            "   Precision: 0.0776\n",
            "   F1-Score:  0.1435\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.0775.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 3/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.8799\n",
            "   IoU:  0.1172\n",
            "   Dice: 0.2084\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.8625\n",
            "   IoU:       0.1505 ✅ NEW BEST!\n",
            "   Dice:      0.2609\n",
            "   Recall:    0.9971\n",
            "   Precision: 0.1506\n",
            "   F1-Score:  0.2609\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.1505.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 4/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.8638\n",
            "   IoU:  0.1934\n",
            "   Dice: 0.3223\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.8503\n",
            "   IoU:       0.2472 ✅ NEW BEST!\n",
            "   Dice:      0.3953\n",
            "   Recall:    0.9959\n",
            "   Precision: 0.2474\n",
            "   F1-Score:  0.3953\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.2472.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 5/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.8509\n",
            "   IoU:  0.2748\n",
            "   Dice: 0.4272\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.8362\n",
            "   IoU:       0.3277 ✅ NEW BEST!\n",
            "   Dice:      0.4922\n",
            "   Recall:    0.9939\n",
            "   Precision: 0.3283\n",
            "   F1-Score:  0.4922\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.3277.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 6/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.8393\n",
            "   IoU:  0.3374\n",
            "   Dice: 0.5005\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.8258\n",
            "   IoU:       0.3941 ✅ NEW BEST!\n",
            "   Dice:      0.5639\n",
            "   Recall:    0.9917\n",
            "   Precision: 0.3954\n",
            "   F1-Score:  0.5639\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.3941.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 7/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.8245\n",
            "   IoU:  0.4123\n",
            "   Dice: 0.5795\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.8122\n",
            "   IoU:       0.4699 ✅ NEW BEST!\n",
            "   Dice:      0.6383\n",
            "   Recall:    0.9887\n",
            "   Precision: 0.4725\n",
            "   F1-Score:  0.6383\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.4699.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 8/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.8114\n",
            "   IoU:  0.4516\n",
            "   Dice: 0.6181\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.8001\n",
            "   IoU:       0.4846 ✅ NEW BEST!\n",
            "   Dice:      0.6513\n",
            "   Recall:    0.9904\n",
            "   Precision: 0.4869\n",
            "   F1-Score:  0.6513\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.4846.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 9/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.7956\n",
            "   IoU:  0.5171\n",
            "   Dice: 0.6799\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.7840\n",
            "   IoU:       0.5603 ✅ NEW BEST!\n",
            "   Dice:      0.7169\n",
            "   Recall:    0.9870\n",
            "   Precision: 0.5646\n",
            "   F1-Score:  0.7169\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " Model saved: best_model_iou_0.5603.pth\n",
            "\n",
            "================================================================================\n",
            "Epoch 10/10\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training:\n",
            "   Loss: 0.7815\n",
            "   IoU:  0.5140\n",
            "   Dice: 0.6737\n",
            "\n",
            "Validation:\n",
            "   Loss:      0.7700\n",
            "   IoU:       0.5501 \n",
            "   Dice:      0.7087\n",
            "   Recall:    0.9855\n",
            "   Precision: 0.5547\n",
            "   F1-Score:  0.7087\n",
            "\n",
            " LR: 0.000100\n",
            "\n",
            " No improvement for 1 epoch(s)\n",
            "\n",
            "================================================================================\n",
            "Final evaluation on test set...\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                           "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Results:\n",
            "   IoU:       0.5297\n",
            "   Dice:      0.6915\n",
            "   Recall:    0.9783\n",
            "   Precision: 0.5358\n",
            "   F1-Score:  0.6915\n",
            "   Accuracy:  0.9752\n",
            "\n",
            "================================================================================\n",
            " TRAINING COMPLETED!\n",
            "   Best Validation IoU: 0.5603 (epoch 9)\n",
            "   Test IoU: 0.5297\n",
            "   Model saved as: best_model_iou_0.5603.pth\n",
            "================================================================================\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ARRwxkmA0i52"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def visualize_prediction(img:np.array, prediction, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Wizualizacja predykcji\n",
        "\n",
        "    Args:\n",
        "        image_path: ścieżka do oryginalnego obrazu\n",
        "        prediction: maska prawdopodobieństwa [H, W]\n",
        "        threshold: próg binaryzacji (default: 0.5)\n",
        "    \"\"\"\n",
        "    # Wczytaj oryginalny obraz\n",
        "    image = img\n",
        "    # np.array(Image.open(image_path))\n",
        "\n",
        "    # Binaryzacja predykcji\n",
        "    binary_mask = (prediction > threshold).astype(np.uint8)\n",
        "\n",
        "    # Wizualizacja\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # Oryginalny obraz\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=14)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Heatmapa prawdopodobieństwa\n",
        "    im1 = axes[1].imshow(prediction, cmap='hot', vmin=0, vmax=1)\n",
        "    axes[1].set_title(\"Probability Heatmap\", fontsize=14)\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
        "\n",
        "    # Binarna maska\n",
        "    axes[2].imshow(binary_mask, cmap='gray', vmin=0, vmax=1)\n",
        "    axes[2].set_title(f\"Binary Mask (threshold={threshold})\", fontsize=14)\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Overlay\n",
        "    axes[3].imshow(image)\n",
        "    axes[3].imshow(prediction, cmap='Reds', alpha=0.5, vmin=0, vmax=1)\n",
        "    axes[3].set_title(\"Overlay\", fontsize=14)\n",
        "    axes[3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statystyki\n",
        "    crack_pixels = binary_mask.sum()\n",
        "    total_pixels = binary_mask.size\n",
        "    crack_percentage = (crack_pixels / total_pixels) * 100\n",
        "\n",
        "    print(f\"📊 Statistics:\")\n",
        "    print(f\"   Crack pixels: {crack_pixels:,} ({crack_percentage:.2f}% of image)\")\n",
        "    print(f\"   Max probability: {prediction.max():.3f}\")\n",
        "    print(f\"   Mean probability: {prediction.mean():.3f}\")\n"
      ],
      "metadata": {
        "id": "eAoqVXQL_tQc"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = torch.load(\"best_model_iou_0.5603.pth\", weights_only=False)\n",
        "\n",
        "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "# visualize_prediction(image_path, prediction, threshold=0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9trnC4he_-ti",
        "outputId": "a3581135-1f73-45bb-c669-8a34cdd71481"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "testing_dataset = get_dataset(\"/content/DeepCrack/test_img\",\"/content/DeepCrack/test_lab\" )"
      ],
      "metadata": {
        "id": "w2iA-HYbAWkn"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y1ghq4XWFudk"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_-C8SCkFGOpB"
      },
      "execution_count": 141,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}