{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZXTX1HXuOdJ",
        "outputId": "4c0b74de-819d-4547-d5e0-4f5e1569817a",
        "tags": [
          "command"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'bachlor_google'...\n",
            "remote: Enumerating objects: 1127, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "^Cceiving objects:   7% (79/1127), 13.28 MiB | 6.36 MiB/s\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/mszczesniak02/bachlor_google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcFdDSJruOdK",
        "tags": [
          "command"
        ]
      },
      "outputs": [],
      "source": [
        "!cp -r /content/bachlor_google/DeepCrack/ ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3abHpmtZuOdL",
        "outputId": "975c03f2-1de2-4b62-dc17-87eebc0d60a9",
        "tags": [
          "command"
        ]
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting segmentation-models-pytorch\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (11.3.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.6.2)\n",
            "Requirement already satisfied: timm>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (1.0.20)\n",
            "Requirement already satisfied: torch>=1.8 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from segmentation-models-pytorch) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.10.5)\n",
            "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segmentation-models-pytorch\n",
            "Successfully installed segmentation-models-pytorch-0.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install segmentation-models-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "--P1NscFuOdM"
      },
      "outputs": [],
      "source": [
        "import albumentations as A                              # for augmentation transform\n",
        "\n",
        "import numpy as np                                      # sci kit specials ;D\n",
        "import matplotlib.pyplot as plt                         # plots\n",
        "from PIL import Image                                   # for opening images as numpy arrays or torch tensors\n",
        "\n",
        "from datetime import datetime                           # for model timestamp\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset                    # preset class for creating a dataset\n",
        "from torch.utils.data import random_split               # for splitting datasets into training, test, validation\n",
        "from torch.utils.data import DataLoader                 # self-explanitory\n",
        "import segmentation_models_pytorch as smp               # preset model\n",
        "\n",
        "from tqdm import tqdm                                   # for the progress bar\n",
        "import os                                               # for accessing files and setting proper paths to   them\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter       # tensorboard srv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XvxDfZEXuOdO"
      },
      "outputs": [],
      "source": [
        "DEBUG = True\n",
        "\n",
        "if DEBUG==True:\n",
        "\n",
        "  MASK_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_lab\"\n",
        "  IMG_TRAIN_PATH = \"../assets/datasets/DeepCrack/train_img\"\n",
        "  MASK_TEST_PATH = \"../assets/datasets/DeepCrack/test_lab\"\n",
        "  IMG_TEST_PATH = \"../assets/datasets/DeepCrack/test_img\"\n",
        "  DEVICE = \"cpu\"\n",
        "  WORKERS = 4\n",
        "\n",
        "else:\n",
        "  MASK_TRAIN_PATH = \"/content/DeepCrack/train_lab\"\n",
        "  IMG_TRAIN_PATH = \"/content/DeepCrack/train_img\"\n",
        "  MASK_TEST_PATH = \"/content/DeepCrack/test_lab\"\n",
        "  IMG_TEST_PATH = \"/content/DeepCrack/test_img\"\n",
        "\n",
        "  DEVICE=\"cuda\"\n",
        "  WORKERS = 4\n",
        "\n",
        "BATCH_SIZE = 2\n",
        "PIN_MEMORY = True\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS = 1\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = 15\n",
        "\n",
        "SCHEDULER_PATIENCE = 5\n",
        "SCHEDULER_FACTOR = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "yshAUFtwuOdO"
      },
      "outputs": [],
      "source": [
        "def fetch_data(path) -> list:\n",
        "  \"\"\"Return files as their paths+filename in an array\"\"\"\n",
        "\n",
        "  assert (os.path.exists(path) == True),  \"Failure during data fetching\"\n",
        "\n",
        "  result = []\n",
        "  for file in tqdm(os.listdir(path), desc=f\"Loading files from {path} \",unit=\"File\", leave=True):\n",
        "    fpath = os.path.join(path,file)\n",
        "    result.append(fpath)\n",
        "\n",
        "  print(f\"{path} - len({len(result)})\")\n",
        "  return result\n",
        "\n",
        "\n",
        "class DeepCrackDataset(Dataset):\n",
        "  def __init__(self, img_dir, mask_dir, transform=None):\n",
        "\n",
        "    self.img_dir = img_dir\n",
        "    self.mask_dir = mask_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    # sort values so the file names corespoding to each other are loaded in order\n",
        "    self.images = sorted([os.path.join(img_dir, file) for file in os.listdir(img_dir)] )\n",
        "    self.masks = sorted([os.path.join(mask_dir, file) for file in os.listdir(mask_dir)])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    np_image = np.array(Image.open(self.images[index]))\n",
        "    np_mask = np.array(Image.open(self.masks[index]))\n",
        "\n",
        "\n",
        "    if len(np_mask.shape) == 3:\n",
        "      np_mask = np_mask[:,:,0]\n",
        "\n",
        "    np_mask = (np_mask > 127).astype(np.uint8)\n",
        "\n",
        "    if self.transform: # if using transforms\n",
        "      t = self.transform(image=np_image, mask=np_mask)\n",
        "      np_image = t[\"image\"]\n",
        "      np_mask = t[\"mask\"]\n",
        "\n",
        "    # conversion from numpy array convention to tensor via permute,\n",
        "    #     then normalizing to [0,1] range, same for mask, only using binary data\n",
        "    tensor_image = torch.from_numpy(np_image).permute(2, 0, 1).float() / 255.0\n",
        "    tensor_mask = torch.from_numpy(np_mask).unsqueeze(0).float()\n",
        "\n",
        "    return tensor_image,tensor_mask\n",
        "\n",
        "\n",
        "def get_dataset(img_path = IMAGE_PATH, mask_path = MASK_PATH, transform_train = None ):\n",
        "\n",
        "  dataset = DeepCrackDataset(img_path, mask_path, transform=transform_train)\n",
        "  return dataset\n",
        "\n",
        "def split_dataset(dataset: DeepCrackDataset, test_factor:float, val_factor:float )->list:\n",
        "  \"\"\"Split exising dataset given percentages as [0,1] floats, return list of  \"\"\"\n",
        "  return random_split(dataset, [test_factor, val_factor])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "W7UY0hh3uOdP"
      },
      "outputs": [],
      "source": [
        "transofrm_train = A.Compose([\n",
        "    A.Resize(512, 512),  # ← Stały rozmiar, bez crop\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.3),\n",
        "    A.Rotate(limit=10, p=0.3),  # ← Mniejszy kąt\n",
        "    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.3),  # ← Mniej agresywne\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xrXVv0mLuOdQ"
      },
      "outputs": [],
      "source": [
        "class DiceLoss(torch.nn.Module):\n",
        "  def __init__(self, smooth=1e-6):\n",
        "    super(DiceLoss,self).__init__()\n",
        "    self.smooth = smooth\n",
        "  def forward(self, predictions, targets):\n",
        "    predictions = torch.sigmoid(predictions)\n",
        "\n",
        "    predictions = predictions.view(-1)\n",
        "    targets = targets.view(-1)\n",
        "\n",
        "    intersection = (predictions * targets).sum()\n",
        "    dice = (2. * intersection  + self.smooth) / (predictions.sum() + targets.sum() + self.smooth)\n",
        "\n",
        "    return 1-dice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xOyfdE20uOdS"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(predictions, targets, threshold=0.5):\n",
        "\n",
        "    # Binaryzacja\n",
        "    preds = (predictions > threshold).float()\n",
        "    targets = targets.float()\n",
        "\n",
        "    # Flatten\n",
        "    preds_flat = preds.view(-1)\n",
        "    targets_flat = targets.view(-1)\n",
        "\n",
        "    # True/False Positives/Negatives\n",
        "    TP = ((preds_flat == 1) & (targets_flat == 1)).sum().float()\n",
        "    TN = ((preds_flat == 0) & (targets_flat == 0)).sum().float()\n",
        "    FP = ((preds_flat == 1) & (targets_flat == 0)).sum().float()\n",
        "    FN = ((preds_flat == 0) & (targets_flat == 1)).sum().float()\n",
        "\n",
        "    # Metryki\n",
        "    epsilon = 1e-7  # Unikaj dzielenia przez zero\n",
        "\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN + epsilon)\n",
        "    precision = TP / (TP + FP + epsilon)\n",
        "    recall = TP / (TP + FN + epsilon)\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "    specificity = TN / (TN + FP + epsilon)\n",
        "\n",
        "    # IoU (Intersection over Union) - NAJWAŻNIEJSZA dla segmentacji!\n",
        "    intersection = (preds * targets).sum()\n",
        "    union = preds.sum() + targets.sum() - intersection\n",
        "    iou = intersection / (union + epsilon)\n",
        "\n",
        "    # Dice Coefficient\n",
        "    dice = (2 * intersection) / (preds.sum() + targets.sum() + epsilon)\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy.item(),\n",
        "        'precision': precision.item(),\n",
        "        'recall': recall.item(),\n",
        "        'f1_score': f1_score.item(),\n",
        "        'specificity': specificity.item(),\n",
        "        'iou': iou.item(),\n",
        "        'dice': dice.item(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "N3IY_qYBuOdT"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = .0\n",
        "    metrics = {\n",
        "        'iou': [], 'dice': [], 'recall': [],\n",
        "        'precision': [], 'f1_score': []\n",
        "    }\n",
        "    loop = tqdm(train_loader, desc=\"Training\", leave=False)\n",
        "\n",
        "    for batch_idx, (images, masks) in enumerate(loop):\n",
        "        # move to adequete memory\n",
        "        images = images.to(device)\n",
        "        masks = masks.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(images)\n",
        "        loss = criterion(predictions, masks)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions_sigmoid = torch.sigmoid(predictions)\n",
        "            batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "            for key in metrics.keys():\n",
        "                metrics[key].append(batch_metrics[key])\n",
        "\n",
        "        loop.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    loop.close()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "def validate(model, val_loader, criterion,device):\n",
        "  model.eval()\n",
        "  running_loss = 0.0\n",
        "  metrics = {\n",
        "      'iou': [], 'dice': [], 'recall': [],\n",
        "      'precision': [], 'f1_score': [], 'accuracy': []\n",
        "      }\n",
        "  with torch.no_grad():\n",
        "    for images,masks in tqdm(val_loader, desc=\"Validation\", leave=False):\n",
        "      images = images.to(device)\n",
        "      masks = masks.to(device)\n",
        "\n",
        "      predictions = model(images)\n",
        "      loss = criterion(predictions, masks)\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      predictions_sigmoid = torch.sigmoid(predictions)\n",
        "      batch_metrics = calculate_metrics(predictions_sigmoid, masks)\n",
        "\n",
        "      for key in metrics.keys():\n",
        "          metrics[key].append(batch_metrics[key])\n",
        "\n",
        "    avg_loss = running_loss / len(val_loader)\n",
        "    avg_metrics = {k: np.mean(v) for k, v in metrics.items()}\n",
        "    avg_metrics['loss'] = avg_loss\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    all_ground_truths = []\n",
        "\n",
        "    print(\"Running evaluation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, masks in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "            images = images.to(device)\n",
        "            masks = masks.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            predictions = torch.sigmoid(outputs)\n",
        "\n",
        "            # Do CPU\n",
        "            all_predictions.append(predictions.cpu().numpy())\n",
        "            all_ground_truths.append(masks.cpu().numpy())\n",
        "\n",
        "    # Concatenate\n",
        "    predictions = np.concatenate(all_predictions, axis=0)[:, 0]  # [N, H, W]\n",
        "    ground_truths = np.concatenate(all_ground_truths, axis=0)[:, 0]  # [N, H, W]\n",
        "\n",
        "    # Binaryzacja\n",
        "    pred_binary = (predictions > threshold).astype(np.float32)\n",
        "    gt_binary = (ground_truths > 0.5).astype(np.float32)\n",
        "\n",
        "    # ========================================\n",
        "    # OBLICZ METRYKI\n",
        "    # ========================================\n",
        "    ious = []\n",
        "    precisions = []\n",
        "    recalls = []\n",
        "    f1_scores = []\n",
        "    accuracies = []\n",
        "\n",
        "    for pred, gt in zip(pred_binary, gt_binary):\n",
        "        # Confusion matrix\n",
        "        tp = (pred * gt).sum()\n",
        "        fp = (pred * (1 - gt)).sum()\n",
        "        fn = ((1 - pred) * gt).sum()\n",
        "        tn = ((1 - pred) * (1 - gt)).sum()\n",
        "\n",
        "        # IoU (Intersection over Union)\n",
        "        iou = tp / (tp + fp + fn + 1e-6)\n",
        "        ious.append(iou)\n",
        "\n",
        "        # Precision (jakość detekcji)\n",
        "        precision = tp / (tp + fp + 1e-6)\n",
        "        precisions.append(precision)\n",
        "\n",
        "        # Recall (czułość)\n",
        "        recall = tp / (tp + fn + 1e-6)\n",
        "        recalls.append(recall)\n",
        "\n",
        "        # F1 Score (harmonic mean)\n",
        "        f1 = 2 * (precision * recall) / (precision + recall + 1e-6)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = (tp + tn) / (tp + tn + fp + fn + 1e-6)\n",
        "        accuracies.append(accuracy)\n",
        "\n",
        "    metrics = {\n",
        "        'iou_mean': np.mean(ious),\n",
        "        'iou_std': np.std(ious),\n",
        "        'iou_median': np.median(ious),\n",
        "        'precision_mean': np.mean(precisions),\n",
        "        'precision_std': np.std(precisions),\n",
        "        'recall_mean': np.mean(recalls),\n",
        "        'recall_std': np.std(recalls),\n",
        "        'f1_mean': np.mean(f1_scores),\n",
        "        'f1_std': np.std(f1_scores),\n",
        "        'accuracy_mean': np.mean(accuracies),\n",
        "        'ious': ious,\n",
        "        'precisions': precisions,\n",
        "        'recalls': recalls,\n",
        "        'f1_scores': f1_scores,\n",
        "    }\n",
        "\n",
        "    return metrics, predictions, ground_truths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MajrjmMsuOdU"
      },
      "outputs": [],
      "source": [
        "def main()-> int:\n",
        "    \n",
        "    its_training_time = datetime.now().strftime('H%M') # Jared Leto likes it \n",
        "    writer = SummaryWriter(f\"runs/model_{its_training_time}\")\n",
        "\n",
        "    train_set = get_dataset(IMG_TRAIN_PATH, MASK_TRAIN_PATH, transform_train=transofrm_train)\n",
        "    testing_dataset = get_dataset(IMG_TEST_PATH, MASK_TEST_PATH, transform_train=None)\n",
        "\n",
        "    \n",
        "    test_set, val_set = split_dataset(testing_dataset, .5, .5 )\n",
        "\n",
        "    train_loader = DataLoader( train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    test_loader = DataLoader( test_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=WORKERS, pin_memory=PIN_MEMORY)\n",
        "    val_loader = DataLoader( val_set, batch_size=BATCH_SIZE , shuffle=False, num_workers=WORKERS    , pin_memory=PIN_MEMORY)\n",
        "\n",
        "    print(\"Datasets sizes: \")\n",
        "    print(f\"\\t  Train: {len(train_set)}\")\n",
        "    print(f\"\\t    Val: {len(val_set)}\")\n",
        "    print(f\"\\t   Test: {len(test_set)}\")\n",
        "\n",
        "    device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "    print(f\"\\t    Total parameters: {total_params:,}\")\n",
        "    print(f\"\\tTrainable parameters: {trainable_params:,}\")\n",
        "    print(f\"\\t          Model size: ~{total_params * 4 / 1e6:.1f} MB\")\n",
        "\n",
        "    criterion = DiceLoss()\n",
        "    optimizer = torch.optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=LEARNING_RATE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "    )\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',  # Maksymalizuj IoU\n",
        "        factor=SCHEDULER_FACTOR,\n",
        "        patience=SCHEDULER_PATIENCE,\n",
        "        min_lr=1e-7\n",
        "    )\n",
        "\n",
        "    print(f\"\\nTraining configuration:\")\n",
        "    print(f\"\\t       Optimizer: Adam\")\n",
        "    print(f\"\\t   Learning rate: {LEARNING_RATE}\")\n",
        "    print(f\"\\t    Weight decay: {WEIGHT_DECAY}\")\n",
        "    print(f\"\\t       Scheduler: ReduceLROnPlateau (patience={SCHEDULER_PATIENCE})\")\n",
        "    print(f\"\\t  Early stopping: patience={EARLY_STOPPING_PATIENCE}\")\n",
        "    print(f\"\\t          Epochs: {EPOCHS}\")\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    best_val_iou = 0.0\n",
        "    best_epoch = 0\n",
        "    patience_counter = 0\n",
        "\n",
        "    writer.add_text(\"Hparams\",f\"\"\"\n",
        "    -           Learning Rate: {LEARNING_RATE}\n",
        "    -              Batch Size: {BATCH_SIZE}\n",
        "    -            Weight Decay: {WEIGHT_DECAY}\n",
        "    -                  Epochs: {EPOCHS}\n",
        "    -      Scheduler Patience: {SCHEDULER_PATIENCE}\n",
        "    - Early Stopping Patience: {EARLY_STOPPING_PATIENCE}\n",
        "    -                  Device: {DEVICE}\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        \n",
        "        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        \n",
        "        val_metrics = validate(model, val_loader, criterion, device)\n",
        "\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        scheduler.step(val_metrics['iou'])\n",
        "\n",
        "        # Loss\n",
        "        writer.add_scalars('Loss', {\n",
        "            'train': train_metrics['loss'],\n",
        "            'val': val_metrics['loss']\n",
        "        }, epoch)\n",
        "        \n",
        "        # IoU\n",
        "        writer.add_scalars('IoU', {\n",
        "            'train': train_metrics['iou'],\n",
        "            'val': val_metrics['iou']\n",
        "        }, epoch)\n",
        "        \n",
        "        # Dice\n",
        "        writer.add_scalars('Dice', {\n",
        "            'train': train_metrics['dice'],\n",
        "            'val': val_metrics['dice']\n",
        "        }, epoch)\n",
        "        \n",
        "        # Precision\n",
        "        writer.add_scalars('Precision', {\n",
        "            'train': train_metrics['precision'],\n",
        "            'val': val_metrics['precision']\n",
        "        }, epoch)\n",
        "        \n",
        "        # Recall\n",
        "        writer.add_scalars('Recall', {\n",
        "            'train': train_metrics['recall'],\n",
        "            'val': val_metrics['recall']\n",
        "        }, epoch)\n",
        "        \n",
        "        # F1-Score\n",
        "        writer.add_scalars('F1-Score', {\n",
        "            'train': train_metrics['f1_score'],\n",
        "            'val': val_metrics['f1_score']\n",
        "        }, epoch)\n",
        "        \n",
        "        # Learning Rate\n",
        "        writer.add_scalar('Learning_Rate', current_lr, epoch)\n",
        "        \n",
        "        # Accuracy (tylko validation)\n",
        "        writer.add_scalar('Validation/Accuracy', val_metrics['accuracy'], epoch)\n",
        "\n",
        "       # ========================================\n",
        "        # PRINT METRICS\n",
        "        # ========================================\n",
        "        # print(f\"\\nTraining:\")\n",
        "        # print(f\"   Loss: {train_metrics['loss']:.4f}\")\n",
        "        # print(f\"   IoU:  {train_metrics['iou']:.4f}\")\n",
        "        # print(f\"   Dice: {train_metrics['dice']:.4f}\")\n",
        "\n",
        "        # print(f\"\\nValidation:\")\n",
        "        # print(f\"   Loss:      {val_metrics['loss']:.4f}\")\n",
        "        # print(f\"   IoU:       {val_metrics['iou']:.4f} {'✅ NEW BEST!' if val_metrics['iou'] > best_val_iou else ''}\")\n",
        "        # print(f\"   Dice:      {val_metrics['dice']:.4f}\")\n",
        "        # print(f\"   Recall:    {val_metrics['recall']:.4f}\")\n",
        "        # print(f\"   Precision: {val_metrics['precision']:.4f}\")\n",
        "        # print(f\"   F1-Score:  {val_metrics['f1_score']:.4f}\")\n",
        "\n",
        "        # print(f\"\\n LR: {current_lr:.6f}\")\n",
        "\n",
        "        \n",
        "        # SAVE BEST MODEL\n",
        "        if val_metrics['iou'] > best_val_iou:\n",
        "            best_val_iou = val_metrics['iou']\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            checkpoint = {\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'best_val_iou': best_val_iou,\n",
        "                'val_metrics': val_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "            }\n",
        "\n",
        "            torch.save(checkpoint, f'best_model_iou_{best_val_iou:.4f}.pth')\n",
        "            print(f\"\\n Model saved: best_model_iou_{best_val_iou:.4f}.pth\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"\\n No improvement for {patience_counter} epoch(s)\")\n",
        "\n",
        "        # ========================================\n",
        "        # EARLY STOPPING\n",
        "        # ========================================\n",
        "        if patience_counter >= EARLY_STOPPING_PATIENCE:\n",
        "            print(f\"\\n  Early stopping triggered!\")\n",
        "            print(f\"   No improvement for {EARLY_STOPPING_PATIENCE} epochs\")\n",
        "            print(f\"   Best IoU: {best_val_iou:.4f} at epoch {best_epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            val_images, val_masks = next(iter(val_loader))\n",
        "            val_images = val_images.to(device)\n",
        "            val_masks = val_masks.to(device)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(val_images)\n",
        "                val_preds = torch.sigmoid(val_outputs)\n",
        "            \n",
        "            # Weź pierwsze 4 obrazy\n",
        "            writer.add_images('Images/Input', val_images[:4], epoch)\n",
        "            writer.add_images('Images/Ground_Truth', val_masks[:4], epoch)\n",
        "            writer.add_images('Images/Prediction', val_preds[:4], epoch)\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Failure - Could not log images: {e}\")\n",
        " \n",
        "    # ========================================\n",
        "    # FINAL EVALUATION ON TEST SET\n",
        "    # ========================================\n",
        "\n",
        "    writer.add_text('Final_Test_Metrics', f\"\"\"\n",
        "    Test Set Results (Best Model from Epoch {best_epoch + 1})\n",
        "    \n",
        "-       IoU: {test_metrics['iou']:.4f}\n",
        "-      Dice: {test_metrics['dice']:.4f}\n",
        "-    Recall: {test_metrics['recall']:.4f}\n",
        "- Precision: {test_metrics['precision']:.4f}\n",
        "-  F1-Score: {test_metrics['f1_score']:.4f}\n",
        "-  Accuracy: {test_metrics['accuracy']:.4f}\n",
        "Best Validation IoU: {best_val_iou:.4f}\n",
        "    \"\"\")\n",
        "    \n",
        "    # Dodaj finalne metryki jako skalary\n",
        "    writer.add_scalar('Final/Test_IoU', test_metrics['iou'], 0)\n",
        "    writer.add_scalar('Final/Test_Dice', test_metrics['dice'], 0)\n",
        "    writer.add_scalar('Final/Test_Recall', test_metrics['recall'], 0)\n",
        "    writer.add_scalar('Final/Test_Precision', test_metrics['precision'], 0)\n",
        "    writer.add_scalar('Final/Test_F1', test_metrics['f1_score'], 0)\n",
        "    \n",
        "    # Zapisz hyperparametry vs metryki (do porównania w TensorBoard)\n",
        "    writer.add_hparams(\n",
        "        {\n",
        "            'lr': LEARNING_RATE,\n",
        "            'batch_size': BATCH_SIZE,\n",
        "            'weight_decay': WEIGHT_DECAY,\n",
        "            'scheduler_patience': SCHEDULER_PATIENCE,\n",
        "        },\n",
        "        {\n",
        "            'hparam/test_iou': test_metrics['iou'],\n",
        "            'hparam/test_dice': test_metrics['dice'],\n",
        "            'hparam/test_f1': test_metrics['f1_score'],\n",
        "        }\n",
        "    )\n",
        "\n",
        "   \n",
        "    writer.close()\n",
        "\n",
        "\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"Final evaluation on test set...\")\n",
        "    print(f\"{'=' * 80}\")\n",
        "\n",
        "    # Load best model\n",
        "    checkpoint = torch.load(f'best_model_iou_{best_val_iou:.4f}.pth', weights_only=False)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    test_metrics = validate(model, test_loader, criterion, device)\n",
        "\n",
        "    print(f\"\\nTest Results:\")\n",
        "    print(f\"   IoU:       {test_metrics['iou']:.4f}\")\n",
        "    print(f\"   Dice:      {test_metrics['dice']:.4f}\")\n",
        "    print(f\"   Recall:    {test_metrics['recall']:.4f}\")\n",
        "    print(f\"   Precision: {test_metrics['precision']:.4f}\")\n",
        "    print(f\"   F1-Score:  {test_metrics['f1_score']:.4f}\")\n",
        "    print(f\"   Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\" TRAINING COMPLETED!\")\n",
        "    print(f\"   Best Validation IoU: {best_val_iou:.4f} (epoch {best_epoch + 1})\")\n",
        "    print(f\"   Test IoU: {test_metrics['iou']:.4f}\")\n",
        "    print(f\"   Model saved as: best_model_iou_{best_val_iou:.4f}.pth\")\n",
        "    print(f\"{'=' * 80}\\n\")\n",
        "\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKAh-oh7vQmJ",
        "outputId": "127b9131-5282-4db9-fc62-c26c48007e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets sizes: \n",
            "\t  Train: 300\n",
            "\t    Val: 118\n",
            "\t   Test: 118\n",
            "\t    Total parameters: 24,436,369\n",
            "\tTrainable parameters: 24,436,369\n",
            "\t          Model size: ~97.7 MB\n",
            "\n",
            "Training configuration:\n",
            "\t       Optimizer: Adam\n",
            "\t   Learning rate: 0.0001\n",
            "\t    Weight decay: 1e-05\n",
            "\t       Scheduler: ReduceLROnPlateau (patience=5)\n",
            "\t  Early stopping: patience=15\n",
            "\t          Epochs: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                      \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 82\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     69\u001b[39m     writer.add_text(\u001b[33m\"\u001b[39m\u001b[33mHparams\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[33m    -           Learning Rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLEARNING_RATE\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33m    -              Batch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBATCH_SIZE\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     76\u001b[39m \u001b[33m    -                  Device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[33m\"\"\"\u001b[39m)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m         train_metrics = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m         val_metrics = validate(model, val_loader, criterion, device)\n\u001b[32m     86\u001b[39m         current_lr = optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     13\u001b[39m masks = masks.to(device)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m predictions = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m loss = criterion(predictions, masks)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/segmentation_models_pytorch/base/model.py:67\u001b[39m, in \u001b[36mSegmentationModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_input_shape(x)\n\u001b[32m     66\u001b[39m features = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m decoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m masks = \u001b[38;5;28mself\u001b[39m.segmentation_head(decoder_output)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:168\u001b[39m, in \u001b[36mUnetDecoder.forward\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    166\u001b[39m     height, width = spatial_shapes[i + \u001b[32m1\u001b[39m]\n\u001b[32m    167\u001b[39m     skip_connection = skip_connections[i] \u001b[38;5;28;01mif\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(skip_connections) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     x = \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_connection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:58\u001b[39m, in \u001b[36mUnetDecoderBlock.forward\u001b[39m\u001b[34m(self, feature_map, target_height, target_width, skip_connection)\u001b[39m\n\u001b[32m     56\u001b[39m     feature_map = torch.cat([feature_map, skip_connection], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     57\u001b[39m     feature_map = \u001b[38;5;28mself\u001b[39m.attention1(feature_map)\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m feature_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m feature_map = \u001b[38;5;28mself\u001b[39m.conv2(feature_map)\n\u001b[32m     60\u001b[39m feature_map = \u001b[38;5;28mself\u001b[39m.attention2(feature_map)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/conv.py:548\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m548\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/nn/modules/conv.py:543\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(\n\u001b[32m    533\u001b[39m         F.pad(\n\u001b[32m    534\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    541\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    542\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARRwxkmA0i52"
      },
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "module 'torch' has no attribute 'empty_cache'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m      2\u001b[39m gc.collect()\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty_cache\u001b[49m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/bachlor/lib/python3.12/site-packages/torch/__init__.py:2745\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m   2742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[32m   2743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, \u001b[34m__name__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2745\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmodule \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mAttributeError\u001b[39m: module 'torch' has no attribute 'empty_cache'"
          ]
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAoqVXQL_tQc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def visualize_prediction(img:np.array, prediction, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Wizualizacja predykcji\n",
        "\n",
        "    Args:\n",
        "        image_path: ścieżka do oryginalnego obrazu\n",
        "        prediction: maska prawdopodobieństwa [H, W]\n",
        "        threshold: próg binaryzacji (default: 0.5)\n",
        "    \"\"\"\n",
        "    # Wczytaj oryginalny obraz\n",
        "    image = img\n",
        "    # np.array(Image.open(image_path))\n",
        "\n",
        "    # Binaryzacja predykcji\n",
        "    binary_mask = (prediction > threshold).astype(np.uint8)\n",
        "\n",
        "    # Wizualizacja\n",
        "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
        "\n",
        "    # Oryginalny obraz\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title(\"Original Image\", fontsize=14)\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Heatmapa prawdopodobieństwa\n",
        "    im1 = axes[1].imshow(prediction, cmap='hot', vmin=0, vmax=1)\n",
        "    axes[1].set_title(\"Probability Heatmap\", fontsize=14)\n",
        "    axes[1].axis('off')\n",
        "    plt.colorbar(im1, ax=axes[1], fraction=0.046)\n",
        "\n",
        "    # Binarna maska\n",
        "    axes[2].imshow(binary_mask, cmap='gray', vmin=0, vmax=1)\n",
        "    axes[2].set_title(f\"Binary Mask (threshold={threshold})\", fontsize=14)\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Overlay\n",
        "    axes[3].imshow(image)\n",
        "    axes[3].imshow(prediction, cmap='Reds', alpha=0.5, vmin=0, vmax=1)\n",
        "    axes[3].set_title(\"Overlay\", fontsize=14)\n",
        "    axes[3].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Statystyki\n",
        "    crack_pixels = binary_mask.sum()\n",
        "    total_pixels = binary_mask.size\n",
        "    crack_percentage = (crack_pixels / total_pixels) * 100\n",
        "\n",
        "    print(f\"📊 Statistics:\")\n",
        "    print(f\"   Crack pixels: {crack_pixels:,} ({crack_percentage:.2f}% of image)\")\n",
        "    print(f\"   Max probability: {prediction.max():.3f}\")\n",
        "    print(f\"   Mean probability: {prediction.mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9trnC4he_-ti",
        "outputId": "a3581135-1f73-45bb-c669-8a34cdd71481"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint = torch.load(\"best_model_iou_0.5603.pth\", weights_only=False)\n",
        "\n",
        "device = torch.device(DEVICE if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = smp.Unet(\n",
        "        encoder_name=\"resnet34\",\n",
        "        encoder_weights=\"imagenet\",\n",
        "        in_channels=3,\n",
        "        classes=1,\n",
        "        activation=None,\n",
        "    )\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "# visualize_prediction(image_path, prediction, threshold=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2iA-HYbAWkn"
      },
      "outputs": [],
      "source": [
        "testing_dataset = get_dataset(\"/content/DeepCrack/test_img\",\"/content/DeepCrack/test_lab\" )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1ghq4XWFudk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-C8SCkFGOpB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bachlor",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
